{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 [Train]:   0%|          | 0/1563 [00:00<?, ?it/s]"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Filename: custom_vit_tiny_imagenet.py\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Data augmentation and normalization for training\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(64),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Only normalization for validation and test\n",
    "val_transforms = transforms.Compose([\n",
    "    transforms.Resize(64),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Data loaders\n",
    "batch_size = 64\n",
    "\n",
    "# Update the paths according to your directory structure\n",
    "train_dataset = datasets.ImageFolder(\n",
    "    root='tiny-imagenet-200/train',\n",
    "    transform=train_transforms\n",
    ")\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=4\n",
    ")\n",
    "\n",
    "val_dataset = datasets.ImageFolder(\n",
    "    root='tiny-imagenet-200/val',\n",
    "    transform=val_transforms\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=4\n",
    ")\n",
    "\n",
    "# Number of classes\n",
    "num_classes = 200\n",
    "\n",
    "# Helper function to extract patches\n",
    "def extract_patches(x, patch_size):\n",
    "    \"\"\"\n",
    "    Extract patches from images.\n",
    "    x: (batch_size, channels, height, width)\n",
    "    Returns: (batch_size, num_patches, patch_size*patch_size*channels)\n",
    "    \"\"\"\n",
    "    batch_size, channels, height, width = x.shape\n",
    "    num_patches = (height // patch_size) * (width // patch_size)\n",
    "    x = x.unfold(2, patch_size, patch_size).unfold(3, patch_size, patch_size)\n",
    "    x = x.permute(0, 2, 3, 1, 4, 5).contiguous()\n",
    "    x = x.view(batch_size, num_patches, -1)\n",
    "    return x\n",
    "\n",
    "# Custom cross-patch attention module\n",
    "class CrossPatchAttention(nn.Module):\n",
    "    def __init__(self, emb_dim, num_heads):\n",
    "        super(CrossPatchAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.scale = (emb_dim // num_heads) ** -0.5\n",
    "\n",
    "        self.to_q = nn.Linear(emb_dim, emb_dim)\n",
    "        self.to_k = nn.Linear(emb_dim, emb_dim)\n",
    "        self.to_v = nn.Linear(emb_dim, emb_dim)\n",
    "        self.to_out = nn.Linear(emb_dim, emb_dim)\n",
    "\n",
    "    def forward(self, q_input, kv_input):\n",
    "        B, N_q, D = q_input.shape\n",
    "        _, N_kv, _ = kv_input.shape\n",
    "\n",
    "        q = self.to_q(q_input).view(B, N_q, self.num_heads,\n",
    "                                    D // self.num_heads).transpose(1, 2)\n",
    "        k = self.to_k(kv_input).view(B, N_kv, self.num_heads,\n",
    "                                     D // self.num_heads).transpose(1, 2)\n",
    "        v = self.to_v(kv_input).view(B, N_kv, self.num_heads,\n",
    "                                     D // self.num_heads).transpose(1, 2)\n",
    "\n",
    "        attn_scores = torch.matmul(q, k.transpose(-2, -1)) * self.scale\n",
    "        attn_weights = torch.softmax(attn_scores, dim=-1)\n",
    "\n",
    "        attn_output = torch.matmul(attn_weights, v)\n",
    "        attn_output = attn_output.transpose(1, 2).reshape(B, N_q, D)\n",
    "        out = self.to_out(attn_output)\n",
    "        return out\n",
    "\n",
    "# Vision Transformer Model\n",
    "class VisionTransformer(nn.Module):\n",
    "    def __init__(self, img_size=64, patch_sizes=[8, 16],\n",
    "                 emb_dim=256, num_heads=8, num_classes=200, depth=6):\n",
    "        super(VisionTransformer, self).__init__()\n",
    "        self.emb_dim = emb_dim\n",
    "\n",
    "        # Positional Encodings\n",
    "        num_patches_8 = (img_size // 8) ** 2\n",
    "        num_patches_16 = (img_size // 16) ** 2\n",
    "        self.pos_embed_8 = nn.Parameter(\n",
    "            torch.randn(1, num_patches_8, emb_dim)\n",
    "        )\n",
    "        self.pos_embed_16 = nn.Parameter(\n",
    "            torch.randn(1, num_patches_16, emb_dim)\n",
    "        )\n",
    "\n",
    "        # Patch Embedding Layers\n",
    "        self.patch_embed_8 = nn.Linear(8 * 8 * 3, emb_dim)\n",
    "        self.patch_embed_16 = nn.Linear(16 * 16 * 3, emb_dim)\n",
    "\n",
    "        # Transformer Encoder Layers\n",
    "        self.transformer_blocks = nn.ModuleList([\n",
    "            nn.ModuleList([\n",
    "                nn.LayerNorm(emb_dim),\n",
    "                CrossPatchAttention(emb_dim, num_heads),\n",
    "                nn.LayerNorm(emb_dim),\n",
    "                nn.Sequential(\n",
    "                    nn.Linear(emb_dim, emb_dim * 4),\n",
    "                    nn.GELU(),\n",
    "                    nn.Linear(emb_dim * 4, emb_dim)\n",
    "                )\n",
    "            ]) for _ in range(depth)\n",
    "        ])\n",
    "\n",
    "        # Classification Head\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.LayerNorm(emb_dim),\n",
    "            nn.Linear(emb_dim, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        B = x.size(0)\n",
    "\n",
    "        # Extract patches and embeddings for patch size 8 (Q)\n",
    "        patches_8 = extract_patches(x, patch_size=8)  # Shape: (B, 64, 192)\n",
    "        embeddings_8 = self.patch_embed_8(patches_8)  # Shape: (B, 64, D)\n",
    "        embeddings_8 += self.pos_embed_8.to(x.device)\n",
    "\n",
    "        # Extract patches and embeddings for patch size 16 (K, V)\n",
    "        patches_16 = extract_patches(x, patch_size=16)  # Shape: (B, 16, 768)\n",
    "        embeddings_16 = self.patch_embed_16(patches_16)  # Shape: (B, 16, D)\n",
    "        embeddings_16 += self.pos_embed_16.to(x.device)\n",
    "\n",
    "        q = embeddings_8\n",
    "        kv = embeddings_16\n",
    "\n",
    "        # Pass through transformer blocks\n",
    "        for ln1, attn, ln2, mlp in self.transformer_blocks:\n",
    "            # Custom attention with Q from finer patches and K,V from coarser patches\n",
    "            q = q + attn(ln1(q), ln1(kv))\n",
    "            # MLP\n",
    "            q = q + mlp(ln2(q))\n",
    "\n",
    "        # Pooling (take mean over sequence length)\n",
    "        out = q.mean(dim=1)  # Shape: (B, D)\n",
    "\n",
    "        # Classification head\n",
    "        logits = self.classifier(out)  # Shape: (B, num_classes)\n",
    "        return logits\n",
    "\n",
    "# Accuracy metrics\n",
    "def accuracy(output, target, topk=(1, 3, 5)):\n",
    "    \"\"\"Computes the accuracy over the k top predictions.\"\"\"\n",
    "    maxk = max(topk)\n",
    "    batch_size = target.size(0)\n",
    "\n",
    "    _, pred = output.topk(maxk, dim=1, largest=True, sorted=True)\n",
    "    pred = pred.t()  # Shape: (maxk, batch_size)\n",
    "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "    res = []\n",
    "    for k in topk:\n",
    "        correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n",
    "        res.append((correct_k / batch_size).item())\n",
    "    return res  # Returns a list of accuracies for topk\n",
    "\n",
    "# Training function\n",
    "def train(model, device, train_loader, optimizer, criterion, epoch):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    top1_acc = 0.0\n",
    "    top3_acc = 0.0\n",
    "    top5_acc = 0.0\n",
    "\n",
    "    for batch_idx, (inputs, targets) in enumerate(\n",
    "        tqdm(train_loader, desc=f\"Epoch {epoch} [Train]\")\n",
    "    ):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Metrics\n",
    "        acc1, acc3, acc5 = accuracy(outputs, targets)\n",
    "        running_loss += loss.item()\n",
    "        top1_acc += acc1\n",
    "        top3_acc += acc3\n",
    "        top5_acc += acc5\n",
    "\n",
    "    num_batches = len(train_loader)\n",
    "    avg_loss = running_loss / num_batches\n",
    "    avg_top1_acc = top1_acc / num_batches\n",
    "    avg_top3_acc = top3_acc / num_batches\n",
    "    avg_top5_acc = top5_acc / num_batches\n",
    "\n",
    "    return avg_loss, avg_top1_acc, avg_top3_acc, avg_top5_acc\n",
    "\n",
    "# Validation function\n",
    "def validate(model, device, val_loader, criterion):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    top1_acc = 0.0\n",
    "    top3_acc = 0.0\n",
    "    top5_acc = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in tqdm(val_loader, desc=\"Validation\"):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            # Metrics\n",
    "            acc1, acc3, acc5 = accuracy(outputs, targets)\n",
    "            running_loss += loss.item()\n",
    "            top1_acc += acc1\n",
    "            top3_acc += acc3\n",
    "            top5_acc += acc5\n",
    "\n",
    "    num_batches = len(val_loader)\n",
    "    avg_loss = running_loss / num_batches\n",
    "    avg_top1_acc = top1_acc / num_batches\n",
    "    avg_top3_acc = top3_acc / num_batches\n",
    "    avg_top5_acc = top5_acc / num_batches\n",
    "\n",
    "    return avg_loss, avg_top1_acc, avg_top3_acc, avg_top5_acc\n",
    "\n",
    "# Instantiate model, criterion, optimizer\n",
    "model = VisionTransformer(num_classes=num_classes).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=3e-4)\n",
    "\n",
    "# Learning rate scheduler (optional)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "train_losses, val_losses = [], []\n",
    "train_acc1_list, val_acc1_list = [], []\n",
    "train_acc3_list, val_acc3_list = [], []\n",
    "train_acc5_list, val_acc5_list = [], []\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    train_loss, train_acc1, train_acc3, train_acc5 = train(\n",
    "        model, device, train_loader, optimizer, criterion, epoch\n",
    "    )\n",
    "    val_loss, val_acc1, val_acc3, val_acc5 = validate(\n",
    "        model, device, val_loader, criterion\n",
    "    )\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    train_acc1_list.append(train_acc1)\n",
    "    val_acc1_list.append(val_acc1)\n",
    "    train_acc3_list.append(train_acc3)\n",
    "    val_acc3_list.append(val_acc3)\n",
    "    train_acc5_list.append(train_acc5)\n",
    "    val_acc5_list.append(val_acc5)\n",
    "\n",
    "    print(f\"\\nEpoch {epoch} Summary:\")\n",
    "    print(f\"Train Loss: {train_loss:.4f}, \"\n",
    "          f\"Top-1 Acc: {train_acc1*100:.2f}%, \"\n",
    "          f\"Top-3 Acc: {train_acc3*100:.2f}%, \"\n",
    "          f\"Top-5 Acc: {train_acc5*100:.2f}%\")\n",
    "    print(f\"Val Loss: {val_loss:.4f}, \"\n",
    "          f\"Top-1 Acc: {val_acc1*100:.2f}%, \"\n",
    "          f\"Top-3 Acc: {val_acc3*100:.2f}%, \"\n",
    "          f\"Top-5 Acc: {val_acc5*100:.2f}%\")\n",
    "\n",
    "# Plotting training and validation accuracy\n",
    "epochs = range(1, num_epochs + 1)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(epochs, train_acc1_list, 'b-', label='Train Top-1 Acc')\n",
    "plt.plot(epochs, val_acc1_list, 'r-', label='Val Top-1 Acc')\n",
    "plt.plot(epochs, train_acc3_list, 'b--', label='Train Top-3 Acc')\n",
    "plt.plot(epochs, val_acc3_list, 'r--', label='Val Top-3 Acc')\n",
    "plt.plot(epochs, train_acc5_list, 'b-.', label='Train Top-5 Acc')\n",
    "plt.plot(epochs, val_acc5_list, 'r-.', label='Val Top-5 Acc')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cu12.4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
