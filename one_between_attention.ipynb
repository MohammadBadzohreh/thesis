{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 3481288\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1:   0%|          | 0/1563 [00:00<?, ?it/s]"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from PIL import Image\n",
    "from einops import rearrange\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Paths to the dataset\n",
    "train_dir = 'tiny-imagenet-200/train'\n",
    "val_dir = 'tiny-imagenet-200/val'\n",
    "\n",
    "# Define transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((64, 64)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Custom Dataset for Tiny ImageNet Validation Set\n",
    "class TinyImageNetValDataset(Dataset):\n",
    "    def __init__(self, val_dir, transform=None):\n",
    "        self.transform = transform\n",
    "        self.val_dir = val_dir\n",
    "        self.img_dir = os.path.join(val_dir, 'images')\n",
    "        self.annotations = os.path.join(val_dir, 'val_annotations.txt')\n",
    "        self.data = []\n",
    "\n",
    "        # Read the annotations file and create a list of (image_path, label) tuples\n",
    "        with open(self.annotations, 'r') as f:\n",
    "            for line in f:\n",
    "                tokens = line.strip().split('\\t')\n",
    "                img_name = tokens[0]\n",
    "                label = tokens[1]\n",
    "                img_path = os.path.join(self.img_dir, img_name)\n",
    "                self.data.append((img_path, label))\n",
    "\n",
    "        # Create a mapping from label names to indices\n",
    "        self.label_to_idx = {label: idx for idx, label in enumerate(sorted(set([label for _, label in self.data])))}\n",
    "        self.idx_to_label = {idx: label for label, idx in self.label_to_idx.items()}\n",
    "\n",
    "        # Update labels to indices\n",
    "        self.data = [(img_path, self.label_to_idx[label]) for img_path, label in self.data]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, label = self.data[idx]\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "\n",
    "# Create a mapping from label names to indices for the training set\n",
    "def get_train_label_mapping(train_dir):\n",
    "    classes = sorted(os.listdir(train_dir))\n",
    "    label_to_idx = {label: idx for idx, label in enumerate(classes)}\n",
    "    idx_to_label = {idx: label for label, idx in label_to_idx.items()}\n",
    "    return label_to_idx, idx_to_label\n",
    "\n",
    "# Custom Dataset for Tiny ImageNet Training Set (to ensure consistent label mapping)\n",
    "class TinyImageNetTrainDataset(Dataset):\n",
    "    def __init__(self, train_dir, label_to_idx, transform=None):\n",
    "        self.transform = transform\n",
    "        self.train_dir = train_dir\n",
    "        self.data = []\n",
    "        self.label_to_idx = label_to_idx\n",
    "\n",
    "        # Iterate over each class folder\n",
    "        for label in os.listdir(train_dir):\n",
    "            class_dir = os.path.join(train_dir, label, 'images')\n",
    "            img_files = os.listdir(class_dir)\n",
    "            for img_name in img_files:\n",
    "                img_path = os.path.join(class_dir, img_name)\n",
    "                self.data.append((img_path, self.label_to_idx[label]))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, label = self.data[idx]\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "\n",
    "# Get label mappings\n",
    "train_label_to_idx, train_idx_to_label = get_train_label_mapping(train_dir)\n",
    "\n",
    "# Load datasets\n",
    "train_dataset = TinyImageNetTrainDataset(train_dir, train_label_to_idx, transform=transform)\n",
    "val_dataset = TinyImageNetValDataset(val_dir, transform=transform)\n",
    "\n",
    "# Ensure that the validation set uses the same label mapping as the training set\n",
    "val_dataset.label_to_idx = train_label_to_idx\n",
    "val_dataset.idx_to_label = train_idx_to_label\n",
    "val_dataset.data = [(img_path, train_label_to_idx[val_dataset.idx_to_label[label]]) for img_path, label in val_dataset.data]\n",
    "\n",
    "# Data loaders\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "\n",
    "class TransformerEncoderLayer(nn.Module):\n",
    "    def __init__(self, dim, heads, mlp_dim, dropout, seq_length):\n",
    "        super(TransformerEncoderLayer, self).__init__()\n",
    "        self.seq_length = seq_length\n",
    "        self.norm1 = nn.LayerNorm(dim)\n",
    "        self.attn = nn.MultiheadAttention(embed_dim=dim, num_heads=heads, dropout=dropout)\n",
    "        self.norm2 = nn.LayerNorm(dim)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(dim, mlp_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(mlp_dim, dim),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Self-attention\n",
    "        x_norm = self.norm1(x)\n",
    "        x_attn, _ = self.attn(x_norm.transpose(0, 1), x_norm.transpose(0, 1), x_norm.transpose(0, 1))\n",
    "        x = x + x_attn.transpose(0, 1)\n",
    "\n",
    "        # Feed-forward\n",
    "        x = x + self.mlp(self.norm2(x))\n",
    "        return x\n",
    "\n",
    "class AlternatingViT(nn.Module):\n",
    "    def __init__(self, num_classes=200, dim=256, depth=6, heads=8, mlp_dim=512, dropout=0.1):\n",
    "        super(AlternatingViT, self).__init__()\n",
    "        self.dim = dim\n",
    "        self.depth = depth\n",
    "        self.patch_sizes = [16, 8] * (depth // 2)\n",
    "\n",
    "        # Positional embeddings for different patch sizes\n",
    "        self.pos_embedding_16 = nn.Parameter(torch.randn(1, 16, dim))\n",
    "        self.pos_embedding_64 = nn.Parameter(torch.randn(1, 64, dim))\n",
    "\n",
    "        # Patch embeddings\n",
    "        self.patch_embed_16 = nn.Conv2d(3, dim, kernel_size=16, stride=16)\n",
    "        self.patch_embed_8 = nn.Conv2d(3, dim, kernel_size=8, stride=8)\n",
    "\n",
    "        # Transformer encoder layers\n",
    "        self.transformer_layers = nn.ModuleList()\n",
    "        for i in range(depth):\n",
    "            patch_size = self.patch_sizes[i]\n",
    "            seq_length = (64 // patch_size) ** 2\n",
    "            layer = TransformerEncoderLayer(dim, heads, mlp_dim, dropout, seq_length)\n",
    "            self.transformer_layers.append(layer)\n",
    "\n",
    "        # Classification head\n",
    "        self.to_cls_token = nn.Identity()\n",
    "        self.mlp_head = nn.Sequential(\n",
    "            nn.LayerNorm(dim),\n",
    "            nn.Linear(dim, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        # Initial patch embeddings and positional encodings\n",
    "        x_16 = self.patch_embed_16(x)  # [B, D, 4, 4]\n",
    "        x_16 = x_16.flatten(2).transpose(1, 2)  # [B, 16, D]\n",
    "        x_16 = x_16 + self.pos_embedding_16\n",
    "\n",
    "        x_8 = self.patch_embed_8(x)  # [B, D, 8, 8]\n",
    "        x_8 = x_8.flatten(2).transpose(1, 2)  # [B, 64, D]\n",
    "        x_8 = x_8 + self.pos_embedding_64\n",
    "\n",
    "        # Start with sequence of length 16\n",
    "        seq = x_16\n",
    "\n",
    "        for i, layer in enumerate(self.transformer_layers):\n",
    "            seq_length = seq.size(1)\n",
    "\n",
    "            # Apply transformer layer\n",
    "            seq = layer(seq)\n",
    "\n",
    "            # Rearrangement between layers if necessary\n",
    "            if i < self.depth - 1:\n",
    "                next_seq_length = self.transformer_layers[i + 1].seq_length\n",
    "                if seq_length != next_seq_length:\n",
    "                    seq = self.rearrange_tokens(seq, seq_length, next_seq_length)\n",
    "\n",
    "        # Classification head\n",
    "        seq = self.to_cls_token(seq.mean(dim=1))  # Global average pooling\n",
    "        out = self.mlp_head(seq)\n",
    "        return out\n",
    "\n",
    "    def rearrange_tokens(self, seq, seq_length, next_seq_length):\n",
    "        # Reshape tokens to match next sequence length\n",
    "        if seq_length == 16 and next_seq_length == 64:\n",
    "            # Expand sequence from 16 to 64\n",
    "            seq = rearrange(seq, 'b (h w) d -> b d h w', h=4, w=4)\n",
    "            seq = nn.functional.interpolate(seq, scale_factor=2, mode='nearest')\n",
    "            seq = rearrange(seq, 'b d h w -> b (h w) d')\n",
    "        elif seq_length == 64 and next_seq_length == 16:\n",
    "            # Reduce sequence from 64 to 16\n",
    "            seq = rearrange(seq, 'b (h w) d -> b d h w', h=8, w=8)\n",
    "            seq = nn.functional.avg_pool2d(seq, kernel_size=2, stride=2)\n",
    "            seq = rearrange(seq, 'b d h w -> b (h w) d')\n",
    "        return seq\n",
    "\n",
    "def train(model, device, train_loader, optimizer, criterion, epoch):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for images, labels in tqdm(train_loader, desc=f\"Training Epoch {epoch}\"):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "    print(f\"Epoch [{epoch}], Loss: {running_loss / len(train_loader):.4f}\")\n",
    "\n",
    "def evaluate(model, device, data_loader, mode='Validation'):\n",
    "    model.eval()\n",
    "    correct_top1 = 0\n",
    "    correct_top3 = 0\n",
    "    correct_top5 = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(data_loader, desc=f\"Evaluating {mode}\"):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, pred = outputs.topk(5, 1, largest=True, sorted=True)\n",
    "\n",
    "            total += labels.size(0)\n",
    "            correct = pred.eq(labels.view(-1, 1).expand_as(pred))\n",
    "\n",
    "            correct_top1 += correct[:, :1].sum().item()\n",
    "            correct_top3 += correct[:, :3].sum().item()\n",
    "            correct_top5 += correct[:, :5].sum().item()\n",
    "\n",
    "    top1_acc = 100 * correct_top1 / total\n",
    "    top3_acc = 100 * correct_top3 / total\n",
    "    top5_acc = 100 * correct_top5 / total\n",
    "    print(f\"{mode} Top-1 Accuracy: {top1_acc:.2f}%\")\n",
    "    print(f\"{mode} Top-3 Accuracy: {top3_acc:.2f}%\")\n",
    "    print(f\"{mode} Top-5 Accuracy: {top5_acc:.2f}%\")\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = AlternatingViT(num_classes=200).to(device)\n",
    "\n",
    "print(f\"Number of parameters: {count_parameters(model)}\")\n",
    "\n",
    "# Define optimizer and loss function\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    train(model, device, train_loader, optimizer, criterion, epoch)\n",
    "    evaluate(model, device, train_loader, mode='Training')    # Training accuracy\n",
    "    evaluate(model, device, val_loader, mode='Validation')    # Validation accuracy\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cu12.4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
