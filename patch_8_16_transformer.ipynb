{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\m.badzohreh\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "  0%|          | 0/1563 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Paths to the Tiny ImageNet dataset\n",
    "TRAIN_DIR = 'tiny-imagenet-200/train'\n",
    "VAL_DIR = 'tiny-imagenet-200/val'\n",
    "\n",
    "# Hyperparameters\n",
    "BATCH_SIZE = 64\n",
    "NUM_EPOCHS = 20\n",
    "LEARNING_RATE = 0.001\n",
    "NUM_CLASSES = 200\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "class TinyImageNetDataset(Dataset):\n",
    "    def __init__(self, data, labels, transform=None):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "        self.le = LabelEncoder()\n",
    "        self.le.fit(labels)\n",
    "        self.encoded_labels = self.le.transform(labels)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.data[idx]\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        label = self.encoded_labels[idx]\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, label\n",
    "\n",
    "def load_training_data():\n",
    "    data = []\n",
    "    labels = []\n",
    "    for wnid in os.listdir(TRAIN_DIR):\n",
    "        wnid_dir = os.path.join(TRAIN_DIR, wnid, 'images')\n",
    "        if os.path.isdir(wnid_dir):\n",
    "            for img_file in os.listdir(wnid_dir):\n",
    "                img_path = os.path.join(wnid_dir, img_file)\n",
    "                data.append(img_path)\n",
    "                labels.append(wnid)\n",
    "    return data, labels\n",
    "\n",
    "def load_validation_data():\n",
    "    val_data = []\n",
    "    val_labels = []\n",
    "    annotations_file = os.path.join(VAL_DIR, 'val_annotations.txt')\n",
    "    val_annotations = {}\n",
    "    with open(annotations_file, 'r') as f:\n",
    "        for line in f:\n",
    "            tokens = line.strip().split('\\t')\n",
    "            filename, wnid = tokens[0], tokens[1]\n",
    "            val_annotations[filename] = wnid\n",
    "    images_dir = os.path.join(VAL_DIR, 'images')\n",
    "    for img_file in os.listdir(images_dir):\n",
    "        img_path = os.path.join(images_dir, img_file)\n",
    "        if img_file in val_annotations:\n",
    "            val_data.append(img_path)\n",
    "            val_labels.append(val_annotations[img_file])\n",
    "    return val_data, val_labels\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((64, 64)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.480, 0.448, 0.398],\n",
    "                         std=[0.277, 0.269, 0.282])\n",
    "])\n",
    "\n",
    "# Load training data\n",
    "train_data, train_labels = load_training_data()\n",
    "train_dataset = TinyImageNetDataset(train_data, train_labels, transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n",
    "\n",
    "# Load validation data\n",
    "val_data, val_labels = load_validation_data()\n",
    "\n",
    "# Split validation data into new validation and test sets\n",
    "val_data, test_data, val_labels, test_labels = train_test_split(\n",
    "    val_data, val_labels, test_size=0.5, random_state=42, stratify=val_labels)\n",
    "\n",
    "# Create validation dataset and dataloader\n",
    "val_dataset = TinyImageNetDataset(val_data, val_labels, transform=transform)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
    "\n",
    "# Create test dataset and dataloader\n",
    "test_dataset = TinyImageNetDataset(test_data, test_labels, transform=transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
    "\n",
    "class ViT(nn.Module):\n",
    "    def __init__(self, img_size=64, patch_size1=16, patch_size2=8, num_classes=NUM_CLASSES, dim=512, depth=6, heads=8, mlp_dim=1024):\n",
    "        super(ViT, self).__init__()\n",
    "\n",
    "        # For patch size 16x16\n",
    "        num_patches1 = (img_size // patch_size1) ** 2  # 16 patches\n",
    "        patch_dim1 = 3 * patch_size1 * patch_size1\n",
    "        self.patch_size1 = patch_size1\n",
    "\n",
    "        # For patch size 8x8\n",
    "        num_patches2 = (img_size // patch_size2) ** 2  # 64 patches\n",
    "        patch_dim2 = 3 * patch_size2 * patch_size2\n",
    "        self.patch_size2 = patch_size2\n",
    "\n",
    "        # Linear projection layers\n",
    "        self.to_patch_embedding1 = nn.Linear(patch_dim1, dim)\n",
    "        self.to_patch_embedding2 = nn.Linear(patch_dim2, dim)\n",
    "\n",
    "        # Positional embeddings\n",
    "        self.pos_embedding1 = nn.Parameter(torch.randn(1, num_patches1 * 4 + 1, dim))  # 16 patches * 4 repetitions + 1 [CLS]\n",
    "        self.pos_embedding2 = nn.Parameter(torch.randn(1, num_patches2 + 1, dim))\n",
    "\n",
    "        # Shared [CLS] token\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, dim))\n",
    "\n",
    "        # Shared Transformer Encoder\n",
    "        self.transformer = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model=dim, nhead=heads, dim_feedforward=mlp_dim),\n",
    "            num_layers=depth\n",
    "        )\n",
    "\n",
    "        # Learnable parameters for combining attentions\n",
    "        self.gamma = nn.Parameter(torch.tensor([0.5, 0.5]))\n",
    "\n",
    "        # Classification head\n",
    "        self.mlp_head = nn.Sequential(\n",
    "            nn.LayerNorm(dim),\n",
    "            nn.Linear(dim, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "\n",
    "        # First Pass: Patches of size 16x16\n",
    "        patches1 = x.unfold(2, self.patch_size1, self.patch_size1).unfold(3, self.patch_size1, self.patch_size1)\n",
    "        patches1 = patches1.contiguous().view(B, C, -1, self.patch_size1, self.patch_size1)\n",
    "        patches1 = patches1.permute(0, 2, 1, 3, 4)\n",
    "        patches1 = patches1.reshape(B, -1, 3 * self.patch_size1 * self.patch_size1)\n",
    "\n",
    "        # Repeat each patch 4 times\n",
    "        patches1 = patches1.unsqueeze(2).repeat(1, 1, 4, 1).reshape(B, -1, 3 * self.patch_size1 * self.patch_size1)\n",
    "\n",
    "        # Embedding and adding [CLS] token\n",
    "        tokens1 = self.to_patch_embedding1(patches1)\n",
    "        cls_tokens1 = self.cls_token.expand(B, -1, -1)\n",
    "        tokens1 = torch.cat((cls_tokens1, tokens1), dim=1)\n",
    "        tokens1 += self.pos_embedding1[:, :tokens1.size(1), :]\n",
    "\n",
    "        # Transformer encoding\n",
    "        tokens1 = tokens1.permute(1, 0, 2)\n",
    "        encoded_tokens1 = self.transformer(tokens1).permute(1, 0, 2)\n",
    "        attention1 = encoded_tokens1[:, 1:, :]  # Exclude [CLS] token\n",
    "\n",
    "        # Second Pass: Patches of size 8x8 with custom order\n",
    "        patches2 = x.unfold(2, self.patch_size2, self.patch_size2).unfold(3, self.patch_size2, self.patch_size2)\n",
    "        patches2 = patches2.contiguous().view(B, C, -1, self.patch_size2, self.patch_size2)\n",
    "        patches2 = patches2.permute(0, 2, 1, 3, 4)\n",
    "        patches2 = patches2.reshape(B, -1, 3 * self.patch_size2 * self.patch_size2)\n",
    "\n",
    "        # Reordering patches\n",
    "        order = [0,1,8,9,2,3,10,11,4,5,12,13,6,7,14,15,\n",
    "                 16,17,24,25,18,19,26,27,20,21,28,29,22,23,30,31,\n",
    "                 32,33,40,41,34,35,42,43,36,37,44,45,38,39,46,47,\n",
    "                 48,49,56,57,50,51,58,59,52,53,60,61,54,55,62,63]\n",
    "        order = torch.tensor(order).to(x.device)\n",
    "        patches2 = patches2[:, order, :]\n",
    "\n",
    "        # Embedding and adding [CLS] token\n",
    "        tokens2 = self.to_patch_embedding2(patches2)\n",
    "        cls_tokens2 = self.cls_token.expand(B, -1, -1)\n",
    "        tokens2 = torch.cat((cls_tokens2, tokens2), dim=1)\n",
    "        tokens2 += self.pos_embedding2[:, :tokens2.size(1), :]\n",
    "\n",
    "        # Transformer encoding\n",
    "        tokens2 = tokens2.permute(1, 0, 2)\n",
    "        encoded_tokens2 = self.transformer(tokens2).permute(1, 0, 2)\n",
    "        attention2 = encoded_tokens2[:, 1:, :]  # Exclude [CLS] token\n",
    "\n",
    "        # Combining attentions\n",
    "        gamma = torch.softmax(self.gamma, dim=0)\n",
    "        combined_attention = gamma[0] * attention1 + gamma[1] * attention2\n",
    "\n",
    "        # Aggregating combined attention\n",
    "        pooled = combined_attention.mean(dim=1)  # Mean pooling over sequence length\n",
    "\n",
    "        # Classification head\n",
    "        out = self.mlp_head(pooled)\n",
    "\n",
    "        return out\n",
    "\n",
    "def accuracy(output, target, topk=(1, 3, 5)):\n",
    "    \"\"\"Computes the accuracy over the k top predictions\"\"\"\n",
    "    maxk = max(topk)\n",
    "    batch_size = target.size(0)\n",
    "\n",
    "    _, pred = output.topk(maxk, 1, True, True)\n",
    "    pred = pred.t()\n",
    "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "    res = []\n",
    "    for k in topk:\n",
    "        correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n",
    "        res.append(correct_k.mul_(100.0 / batch_size))\n",
    "    return res\n",
    "\n",
    "def train(model, device, train_loader, criterion, optimizer, epoch):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    top1_acc = 0.0\n",
    "    top3_acc = 0.0\n",
    "    top5_acc = 0.0\n",
    "    total = 0\n",
    "\n",
    "    loop = tqdm(train_loader)\n",
    "    for batch_idx, (inputs, targets) in enumerate(loop):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        acc1, acc3, acc5 = accuracy(outputs, targets, topk=(1, 3, 5))\n",
    "        top1_acc += acc1.item()\n",
    "        top3_acc += acc3.item()\n",
    "        top5_acc += acc5.item()\n",
    "        total += 1\n",
    "\n",
    "        # Update tqdm loop\n",
    "        loop.set_description(f\"Epoch [{epoch}/{NUM_EPOCHS}]\")\n",
    "        loop.set_postfix(loss=(running_loss/total), top1_acc=(top1_acc/total), top3_acc=(top3_acc/total), top5_acc=(top5_acc/total))\n",
    "\n",
    "    return running_loss/total, top1_acc/total, top3_acc/total, top5_acc/total\n",
    "\n",
    "def validate(model, device, val_loader, criterion):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    top1_acc = 0.0\n",
    "    top3_acc = 0.0\n",
    "    top5_acc = 0.0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        loop = tqdm(val_loader)\n",
    "        for batch_idx, (inputs, targets) in enumerate(loop):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            acc1, acc3, acc5 = accuracy(outputs, targets, topk=(1, 3, 5))\n",
    "            top1_acc += acc1.item()\n",
    "            top3_acc += acc3.item()\n",
    "            top5_acc += acc5.item()\n",
    "            total += 1\n",
    "\n",
    "            # Update tqdm loop\n",
    "            loop.set_description(f\"Validation\")\n",
    "            loop.set_postfix(loss=(running_loss/total), top1_acc=(top1_acc/total), top3_acc=(top3_acc/total), top5_acc=(top5_acc/total))\n",
    "\n",
    "    return running_loss/total, top1_acc/total, top3_acc/total, top5_acc/total\n",
    "\n",
    "def test(model, device, test_loader, criterion):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    top1_acc = 0.0\n",
    "    top3_acc = 0.0\n",
    "    top5_acc = 0.0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        loop = tqdm(test_loader)\n",
    "        for batch_idx, (inputs, targets) in enumerate(loop):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            acc1, acc3, acc5 = accuracy(outputs, targets, topk=(1, 3, 5))\n",
    "            top1_acc += acc1.item()\n",
    "            top3_acc += acc3.item()\n",
    "            top5_acc += acc5.item()\n",
    "            total += 1\n",
    "\n",
    "            # Update tqdm loop\n",
    "            loop.set_description(f\"Test\")\n",
    "            loop.set_postfix(loss=(running_loss/total), top1_acc=(top1_acc/total), top3_acc=(top3_acc/total), top5_acc=(top5_acc/total))\n",
    "\n",
    "    avg_loss = running_loss / total\n",
    "    avg_top1_acc = top1_acc / total\n",
    "    avg_top3_acc = top3_acc / total\n",
    "    avg_top5_acc = top5_acc / total\n",
    "\n",
    "    print(f\"Test Loss: {avg_loss:.4f}\")\n",
    "    print(f\"Test Acc@1: {avg_top1_acc:.2f}%\")\n",
    "    print(f\"Test Acc@3: {avg_top3_acc:.2f}%\")\n",
    "    print(f\"Test Acc@5: {avg_top5_acc:.2f}%\")\n",
    "\n",
    "    return avg_loss, avg_top1_acc, avg_top3_acc, avg_top5_acc\n",
    "\n",
    "model = ViT().to(DEVICE)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "train_top1_acc = []\n",
    "val_top1_acc = []\n",
    "train_top3_acc = []\n",
    "val_top3_acc = []\n",
    "train_top5_acc = []\n",
    "val_top5_acc = []\n",
    "\n",
    "best_acc = 0.0\n",
    "\n",
    "for epoch in range(1, NUM_EPOCHS + 1):\n",
    "    train_loss, train_acc1, train_acc3, train_acc5 = train(model, DEVICE, train_loader, criterion, optimizer, epoch)\n",
    "    val_loss, val_acc1, val_acc3, val_acc5 = validate(model, DEVICE, val_loader, criterion)\n",
    "\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    train_top1_acc.append(train_acc1)\n",
    "    val_top1_acc.append(val_acc1)\n",
    "    train_top3_acc.append(train_acc3)\n",
    "    val_top3_acc.append(val_acc3)\n",
    "    train_top5_acc.append(train_acc5)\n",
    "    val_top5_acc.append(val_acc5)\n",
    "\n",
    "    # Save the best model\n",
    "    if val_acc1 > best_acc:\n",
    "        best_acc = val_acc1\n",
    "        torch.save(model.state_dict(), 'best_model.pth')\n",
    "        print(f\"Best model saved with accuracy: {best_acc:.2f}%\")\n",
    "\n",
    "    print(f\"Epoch {epoch}: Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "    print(f\"Train Acc@1: {train_acc1:.2f}%, Val Acc@1: {val_acc1:.2f}%\")\n",
    "    print(f\"Train Acc@3: {train_acc3:.2f}%, Val Acc@3: {val_acc3:.2f}%\")\n",
    "    print(f\"Train Acc@5: {train_acc5:.2f}%, Val Acc@5: {val_acc5:.2f}%\")\n",
    "\n",
    "def plot_metrics(train_metric, val_metric, metric_name):\n",
    "    epochs = range(1, NUM_EPOCHS + 1)\n",
    "    plt.figure()\n",
    "    plt.plot(epochs, train_metric, 'b', label=f'Training {metric_name}')\n",
    "    plt.plot(epochs, val_metric, 'r', label=f'Validation {metric_name}')\n",
    "    plt.title(f'Training and Validation {metric_name}')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel(metric_name)\n",
    "    plt.legend()\n",
    "    plt.savefig(f'{metric_name}.png')\n",
    "    plt.show()\n",
    "\n",
    "# Plot Loss\n",
    "plot_metrics(train_losses, val_losses, 'Loss')\n",
    "\n",
    "# Plot Top-1 Accuracy\n",
    "plot_metrics(train_top1_acc, val_top1_acc, 'Top-1 Accuracy')\n",
    "\n",
    "# Plot Top-3 Accuracy\n",
    "plot_metrics(train_top3_acc, val_top3_acc, 'Top-3 Accuracy')\n",
    "\n",
    "# Plot Top-5 Accuracy\n",
    "plot_metrics(train_top5_acc, val_top5_acc, 'Top-5 Accuracy')\n",
    "\n",
    "print(\"Model Parameters:\")\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(f\"{name}: {param.data.shape}\")\n",
    "\n",
    "# Load the best model\n",
    "model.load_state_dict(torch.load('best_model.pth'))\n",
    "\n",
    "# Evaluate on test set\n",
    "test_loss, test_acc1, test_acc3, test_acc5 = test(model, DEVICE, test_loader, criterion)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cu12.4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
