{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "from torch.utils.data import DataLoader\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_2x2_chunk_order(num_rows=28, num_cols=28):\n",
    "    \"\"\"\n",
    "    Create a custom ordering of patch indices for a (num_rows x num_cols) grid,\n",
    "    chunked in 2x2, in row-major order inside each chunk.\n",
    "\n",
    "    Returns:\n",
    "        reorder_indices: a list (or np.array) of length (num_rows * num_cols)\n",
    "                         with the new ordering (0-based).\n",
    "    \"\"\"\n",
    "    # Create a 28x28 array of indices [0..783]\n",
    "    indices = np.arange(num_rows * num_cols).reshape(num_rows, num_cols)\n",
    "    \n",
    "    reorder_list = []\n",
    "    # We'll move in steps of 2 along rows and columns\n",
    "    # to form 2x2 chunks\n",
    "    for row_block in range(0, num_rows, 2):\n",
    "        for col_block in range(0, num_cols, 2):\n",
    "            # This chunk is 2x2\n",
    "            # top-left\n",
    "            reorder_list.append(indices[row_block, col_block])\n",
    "            # top-right\n",
    "            reorder_list.append(indices[row_block, col_block + 1])\n",
    "            # bottom-left\n",
    "            reorder_list.append(indices[row_block + 1, col_block])\n",
    "            # bottom-right\n",
    "            reorder_list.append(indices[row_block + 1, col_block + 1])\n",
    "    \n",
    "    reorder_array = np.array(reorder_list, dtype=int)\n",
    "    return reorder_array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbed(nn.Module):\n",
    "    \"\"\"\n",
    "    A generic patch embedding module that splits an image into patches,\n",
    "    flattens each patch, and projects it to a desired embedding dimension.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels=3, embed_dim=768, patch_size=16, img_size=224):\n",
    "        super().__init__()\n",
    "        self.patch_size = patch_size\n",
    "        self.img_size = img_size\n",
    "        self.num_patches_h = img_size // patch_size\n",
    "        self.num_patches_w = img_size // patch_size\n",
    "        self.num_patches = self.num_patches_h * self.num_patches_w  # e.g. 196 when patch_size=16\n",
    "        self.proj = nn.Conv2d(in_channels, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: shape [B, 3, H, W]\n",
    "        Return: shape [B, num_patches, embed_dim]\n",
    "        \"\"\"\n",
    "        # Conv2d with kernel_size=patch_size => each patch is turned into a single token\n",
    "        x = self.proj(x)  # shape [B, embed_dim, num_patches_h, num_patches_w]\n",
    "        x = x.flatten(2)  # shape [B, embed_dim, num_patches]\n",
    "        x = x.transpose(1, 2)  # shape [B, num_patches, embed_dim]\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        \n",
    "        self.qkv = nn.Linear(embed_dim, embed_dim * 3)\n",
    "        self.out_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        \n",
    "        self.attn_dropout = nn.Dropout(dropout)\n",
    "        self.proj_dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        \"\"\"\n",
    "        x: [B, N, D] where N is number of tokens, D is embed_dim\n",
    "        \"\"\"\n",
    "        B, N, D = x.shape\n",
    "        assert D == self.embed_dim\n",
    "\n",
    "        qkv = self.qkv(x)  # [B, N, 3D]\n",
    "        qkv = qkv.reshape(B, N, 3, self.num_heads, self.head_dim)\n",
    "        qkv = qkv.permute(2, 0, 3, 1, 4)  # [3, B, num_heads, N, head_dim]\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]  # each: [B, num_heads, N, head_dim]\n",
    "        \n",
    "        # Compute attention scores\n",
    "        attn_logits = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
    "        # attn_logits shape: [B, num_heads, N, N]\n",
    "        if mask is not None:\n",
    "            attn_logits = attn_logits + mask  # mask should be broadcastable\n",
    "\n",
    "        attn = F.softmax(attn_logits, dim=-1)\n",
    "        attn = self.attn_dropout(attn)\n",
    "\n",
    "        out = torch.matmul(attn, v)  # [B, num_heads, N, head_dim]\n",
    "        out = out.permute(0, 2, 1, 3).reshape(B, N, D)\n",
    "        out = self.out_proj(out)\n",
    "        out = self.proj_dropout(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    A standard ViT Transformer block:\n",
    "        - LayerNorm\n",
    "        - MSA\n",
    "        - Add/Skip\n",
    "        - LayerNorm\n",
    "        - MLP\n",
    "        - Add/Skip\n",
    "    \"\"\"\n",
    "    def __init__(self, embed_dim, num_heads, mlp_ratio=4.0, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.attn = MultiHeadSelfAttention(embed_dim, num_heads, dropout=dropout)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        \n",
    "        hidden_dim = int(embed_dim * mlp_ratio)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(embed_dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, embed_dim),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Self-attention\n",
    "        x_norm = self.norm1(x)\n",
    "        attn_out = self.attn(x_norm)\n",
    "        x = x + attn_out  # skip connection\n",
    "\n",
    "        # MLP\n",
    "        x_norm = self.norm2(x)\n",
    "        mlp_out = self.mlp(x_norm)\n",
    "        x = x + mlp_out\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MergeBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Custom block for the 9th layer that merges the two branches:\n",
    "        logits = Q_A K_A^T + Q_B K_B^T\n",
    "        attention = softmax(logits)\n",
    "        out = attention * (V_A + V_B)\n",
    "\n",
    "    Then do skip connections, MLP, etc.\n",
    "    \"\"\"\n",
    "    def __init__(self, embed_dim, num_heads, mlp_ratio=4.0, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "\n",
    "        # We'll create linear layers for Q, K, V for branch A and B\n",
    "        self.qkv_A = nn.Linear(embed_dim, 3 * embed_dim)\n",
    "        self.qkv_B = nn.Linear(embed_dim, 3 * embed_dim)\n",
    "\n",
    "        self.out_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        \n",
    "        self.dropout_attn = nn.Dropout(dropout)\n",
    "        self.dropout_proj = nn.Dropout(dropout)\n",
    "\n",
    "        self.norm1_A = nn.LayerNorm(embed_dim)\n",
    "        self.norm1_B = nn.LayerNorm(embed_dim)\n",
    "\n",
    "        # For the MLP part (shared after merge)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        hidden_dim = int(embed_dim * mlp_ratio)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(embed_dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, embed_dim),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, xA, xB):\n",
    "        \"\"\"\n",
    "        xA: [B, N, D] (branch A tokens after 8 blocks)\n",
    "        xB: [B, N, D] (branch B tokens after 8 blocks)\n",
    "        Returns: [B, N, D] (merged)\n",
    "        \"\"\"\n",
    "        B, N, D = xA.shape\n",
    "        # 1) Norm\n",
    "        xA_norm = self.norm1_A(xA)  # shape [B, N, D]\n",
    "        xB_norm = self.norm1_B(xB)  # shape [B, N, D]\n",
    "\n",
    "        # 2) Project to Q,K,V for each branch\n",
    "        qkv_A = self.qkv_A(xA_norm).view(B, N, 3, self.num_heads, self.head_dim)\n",
    "        qkv_B = self.qkv_B(xB_norm).view(B, N, 3, self.num_heads, self.head_dim)\n",
    "\n",
    "        # qkv_A => [B, N, 3, num_heads, head_dim]\n",
    "        qkv_A = qkv_A.permute(2, 0, 3, 1, 4)  # => [3, B, num_heads, N, head_dim]\n",
    "        qA, kA, vA = qkv_A[0], qkv_A[1], qkv_A[2]\n",
    "\n",
    "        qkv_B = qkv_B.permute(2, 0, 3, 1, 4)  # => [3, B, num_heads, N, head_dim]\n",
    "        qB, kB, vB = qkv_B[0], qkv_B[1], qkv_B[2]\n",
    "\n",
    "        # 3) Compute combined attention logits\n",
    "        #    shape of qA, kA => [B, num_heads, N, head_dim]\n",
    "        logits_A = torch.matmul(qA, kA.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
    "        logits_B = torch.matmul(qB, kB.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
    "\n",
    "        logits = logits_A + logits_B  # [B, num_heads, N, N]\n",
    "\n",
    "        attn = F.softmax(logits, dim=-1)\n",
    "        attn = self.dropout_attn(attn)\n",
    "\n",
    "        # 4) Multiply by (vA + vB)\n",
    "        # shape vA, vB => [B, num_heads, N, head_dim]\n",
    "        vSum = vA + vB\n",
    "        out = torch.matmul(attn, vSum)  # [B, num_heads, N, head_dim]\n",
    "\n",
    "        # 5) Reshape & project\n",
    "        out = out.permute(0, 2, 1, 3).reshape(B, N, D)\n",
    "        out = self.out_proj(out)\n",
    "        out = self.dropout_proj(out)\n",
    "\n",
    "        # 6) Skip connection\n",
    "        # We return a single route, so let's pick xA as the base route \n",
    "        # or you could average xA and xB. \n",
    "        # Or, to be consistent, let's do (xA + xB) for skip. \n",
    "        # It's your design choice. \n",
    "        x_merged = (xA + xB) + out  \n",
    "\n",
    "        # 7) MLP with skip\n",
    "        x_norm = self.norm2(x_merged)\n",
    "        mlp_out = self.mlp(x_norm)\n",
    "        x_merged = x_merged + mlp_out\n",
    "\n",
    "        return x_merged\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTwoBranchViT(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        img_size=224,\n",
    "        embed_dim=256,\n",
    "        num_heads=4,\n",
    "        mlp_ratio=4.0,\n",
    "        num_classes=10,\n",
    "        dropout=0.0,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        \n",
    "        # -----------------------\n",
    "        # Branch A: 8×8 patches\n",
    "        # -----------------------\n",
    "        self.patch_embed_A = PatchEmbed(\n",
    "            in_channels=3, embed_dim=embed_dim, patch_size=8, img_size=img_size\n",
    "        )\n",
    "        self.num_patches_A = (img_size // 8) ** 2  # 28×28=784\n",
    "\n",
    "        # -----------------------\n",
    "        # Branch B: 16×16 patches\n",
    "        # -----------------------\n",
    "        self.patch_embed_B = PatchEmbed(\n",
    "            in_channels=3, embed_dim=embed_dim, patch_size=16, img_size=img_size\n",
    "        )\n",
    "        self.num_patches_B = (img_size // 16) ** 2  # 14×14=196 => repeated 4× => 784\n",
    "\n",
    "        # -----------------------\n",
    "        # Positional Embeddings\n",
    "        # Each route has its own\n",
    "        # -----------------------\n",
    "        self.pos_embed_A = nn.Parameter(torch.zeros(1, self.num_patches_A, embed_dim))\n",
    "        self.pos_embed_B = nn.Parameter(torch.zeros(1, self.num_patches_A, embed_dim))\n",
    "        # note: we store B's pos_embed in shape [1, 784, D], even though we only have 196 unique patches\n",
    "        # but we'll just replicate them. Alternatively, you can only store [1,196,D] and tile 4x at forward().\n",
    "\n",
    "        # -----------------------\n",
    "        # Transformer Blocks 1–8\n",
    "        # (separate for A & B)\n",
    "        # -----------------------\n",
    "        self.blocks_A = nn.ModuleList([\n",
    "            TransformerBlock(embed_dim, num_heads, mlp_ratio, dropout=dropout)\n",
    "            for _ in range(8)\n",
    "        ])\n",
    "        self.blocks_B = nn.ModuleList([\n",
    "            TransformerBlock(embed_dim, num_heads, mlp_ratio, dropout=dropout)\n",
    "            for _ in range(8)\n",
    "        ])\n",
    "\n",
    "        # -----------------------\n",
    "        # Merge Block (9th)\n",
    "        # -----------------------\n",
    "        self.merge_block = MergeBlock(embed_dim, num_heads, mlp_ratio, dropout=dropout)\n",
    "\n",
    "        # -----------------------\n",
    "        # Blocks 10–12 (single route)\n",
    "        # -----------------------\n",
    "        self.blocks_merged = nn.ModuleList([\n",
    "            TransformerBlock(embed_dim, num_heads, mlp_ratio, dropout=dropout)\n",
    "            for _ in range(3)\n",
    "        ])\n",
    "\n",
    "        # -----------------------\n",
    "        # Classification Head\n",
    "        # -----------------------\n",
    "        self.norm_final = nn.LayerNorm(embed_dim)\n",
    "        self.head = nn.Linear(embed_dim, num_classes)\n",
    "\n",
    "        # Init params\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        # Simple initialization\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.trunc_normal_(m.weight, std=0.02)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.zeros_(m.bias)\n",
    "            elif isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.zeros_(m.bias)\n",
    "            elif isinstance(m, nn.LayerNorm):\n",
    "                nn.init.ones_(m.weight)\n",
    "                nn.init.zeros_(m.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # ---------------------------------------\n",
    "        # 1) Branch A (8×8)\n",
    "        # ---------------------------------------\n",
    "         # 1) Extract patches for Branch A\n",
    "        xA = self.patch_embed_A(x)   # shape [B, 784, embed_dim]\n",
    "\n",
    "        # 2) Reorder the patches using your custom indices\n",
    "        #    (make sure `reorder_indices_2x2` is defined beforehand)\n",
    "        xA = xA[:, reorder_indices_2x2, :]\n",
    "\n",
    "        # 3) Add positional embeddings\n",
    "        xA = xA + self.pos_embed_A\n",
    "\n",
    "        # Pass through 8 blocks\n",
    "        for blk in self.blocks_A:\n",
    "            xA = blk(xA)\n",
    "\n",
    "        # ---------------------------------------\n",
    "        # 2) Branch B (16×16)\n",
    "        # ---------------------------------------\n",
    "        xB = self.patch_embed_B(x)  # [B, 196, embed_dim]\n",
    "\n",
    "        # Repeat each patch 4× => shape \n",
    "        \n",
    "        # One approach: repeat the sequence dimension\n",
    "        # xB has shape [B, 196, D]\n",
    "        # We replicate along dim=1 => repeat_interleave(4, dim=1) => [B, 784, D]\n",
    "        xB = xB.repeat_interleave(4, dim=1)\n",
    "\n",
    "        # Add route B's position embedding\n",
    "        xB = xB + self.pos_embed_B  # shape [B, 784, D]\n",
    "\n",
    "        # Pass through 8 blocks\n",
    "        for blk in self.blocks_B:\n",
    "            xB = blk(xB)\n",
    "\n",
    "        # ---------------------------------------\n",
    "        # 3) Merge at block 9\n",
    "        # ---------------------------------------\n",
    "        x_merged = self.merge_block(xA, xB)  # shape [B, 784, D]\n",
    "\n",
    "        # ---------------------------------------\n",
    "        # 4) Blocks 10–12 (single route)\n",
    "        # ---------------------------------------\n",
    "        for blk in self.blocks_merged:\n",
    "            x_merged = blk(x_merged)\n",
    "\n",
    "        # ---------------------------------------\n",
    "        # 5) Classification Head\n",
    "        # ---------------------------------------\n",
    "        # For ViT, common to do e.g. global average pooling\n",
    "        # or use a [CLS] token. We'll do mean-pool here:\n",
    "        x_merged = self.norm_final(x_merged)\n",
    "        # x_merged => [B, 784, D]\n",
    "        x_pooled = x_merged.mean(dim=1)  # [B, D]\n",
    "\n",
    "        logits = self.head(x_pooled)  # [B, num_classes]\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 97\u001b[0m\n\u001b[0;32m     93\u001b[0m             \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  [*] Best model saved.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     95\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining completed. Best accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 97\u001b[0m \u001b[43mtrain_on_cifar10\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[23], line 61\u001b[0m, in \u001b[0;36mtrain_on_cifar10\u001b[1;34m()\u001b[0m\n\u001b[0;32m     59\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m     60\u001b[0m running_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m---> 61\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m     62\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     64\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\utils\\data\\dataloader.py:439\u001b[0m, in \u001b[0;36mDataLoader.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    437\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator\n\u001b[0;32m    438\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 439\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\utils\\data\\dataloader.py:387\u001b[0m, in \u001b[0;36mDataLoader._get_iterator\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    385\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    386\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_worker_number_rationality()\n\u001b[1;32m--> 387\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_MultiProcessingDataLoaderIter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\utils\\data\\dataloader.py:1040\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter.__init__\u001b[1;34m(self, loader)\u001b[0m\n\u001b[0;32m   1033\u001b[0m w\u001b[38;5;241m.\u001b[39mdaemon \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   1034\u001b[0m \u001b[38;5;66;03m# NB: Process.start() actually take some time as it needs to\u001b[39;00m\n\u001b[0;32m   1035\u001b[0m \u001b[38;5;66;03m#     start a process and pass the arguments over via a pipe.\u001b[39;00m\n\u001b[0;32m   1036\u001b[0m \u001b[38;5;66;03m#     Therefore, we only add a worker to self._workers list after\u001b[39;00m\n\u001b[0;32m   1037\u001b[0m \u001b[38;5;66;03m#     it started, so that we do not call .join() if program dies\u001b[39;00m\n\u001b[0;32m   1038\u001b[0m \u001b[38;5;66;03m#     before it starts, and __del__ tries to join but will get:\u001b[39;00m\n\u001b[0;32m   1039\u001b[0m \u001b[38;5;66;03m#     AssertionError: can only join a started process.\u001b[39;00m\n\u001b[1;32m-> 1040\u001b[0m \u001b[43mw\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1041\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_index_queues\u001b[38;5;241m.\u001b[39mappend(index_queue)\n\u001b[0;32m   1042\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_workers\u001b[38;5;241m.\u001b[39mappend(w)\n",
      "File \u001b[1;32mc:\\Users\\m.badzohreh\\AppData\\Local\\anaconda3\\envs\\cu12.4\\Lib\\multiprocessing\\process.py:121\u001b[0m, in \u001b[0;36mBaseProcess.start\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _current_process\u001b[38;5;241m.\u001b[39m_config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdaemon\u001b[39m\u001b[38;5;124m'\u001b[39m), \\\n\u001b[0;32m    119\u001b[0m        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdaemonic processes are not allowed to have children\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    120\u001b[0m _cleanup()\n\u001b[1;32m--> 121\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_Popen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    122\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sentinel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen\u001b[38;5;241m.\u001b[39msentinel\n\u001b[0;32m    123\u001b[0m \u001b[38;5;66;03m# Avoid a refcycle if the target function holds an indirect\u001b[39;00m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;66;03m# reference to the process object (see bpo-30775)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\m.badzohreh\\AppData\\Local\\anaconda3\\envs\\cu12.4\\Lib\\multiprocessing\\context.py:224\u001b[0m, in \u001b[0;36mProcess._Popen\u001b[1;34m(process_obj)\u001b[0m\n\u001b[0;32m    222\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[0;32m    223\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_Popen\u001b[39m(process_obj):\n\u001b[1;32m--> 224\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mProcess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_Popen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\m.badzohreh\\AppData\\Local\\anaconda3\\envs\\cu12.4\\Lib\\multiprocessing\\context.py:337\u001b[0m, in \u001b[0;36mSpawnProcess._Popen\u001b[1;34m(process_obj)\u001b[0m\n\u001b[0;32m    334\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[0;32m    335\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_Popen\u001b[39m(process_obj):\n\u001b[0;32m    336\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpopen_spawn_win32\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Popen\n\u001b[1;32m--> 337\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\m.badzohreh\\AppData\\Local\\anaconda3\\envs\\cu12.4\\Lib\\multiprocessing\\popen_spawn_win32.py:95\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[1;34m(self, process_obj)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     94\u001b[0m     reduction\u001b[38;5;241m.\u001b[39mdump(prep_data, to_child)\n\u001b[1;32m---> 95\u001b[0m     \u001b[43mreduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mto_child\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     97\u001b[0m     set_spawning_popen(\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\m.badzohreh\\AppData\\Local\\anaconda3\\envs\\cu12.4\\Lib\\multiprocessing\\reduction.py:60\u001b[0m, in \u001b[0;36mdump\u001b[1;34m(obj, file, protocol)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdump\u001b[39m(obj, file, protocol\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m     59\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''Replacement for pickle.dump() using ForkingPickler.'''\u001b[39;00m\n\u001b[1;32m---> 60\u001b[0m     \u001b[43mForkingPickler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprotocol\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def train_on_cifar10():\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Hyperparameters\n",
    "    num_epochs = 2\n",
    "    batch_size = 16\n",
    "    learning_rate = 1e-4\n",
    "\n",
    "    # Data transforms\n",
    "    transform_train = T.Compose([\n",
    "        T.Resize(224),\n",
    "        T.RandomHorizontalFlip(),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize((0.4914, 0.4822, 0.4465),\n",
    "                    (0.2023, 0.1994, 0.2010)),\n",
    "    ])\n",
    "    transform_test = T.Compose([\n",
    "        T.Resize(224),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize((0.4914, 0.4822, 0.4465),\n",
    "                    (0.2023, 0.1994, 0.2010)),\n",
    "    ])\n",
    "\n",
    "    # Datasets\n",
    "    train_dataset = torchvision.datasets.CIFAR10(\n",
    "        root='./data',\n",
    "        train=True,\n",
    "        download=True,\n",
    "        transform=transform_train\n",
    "    )\n",
    "    test_dataset = torchvision.datasets.CIFAR10(\n",
    "        root='./data',\n",
    "        train=False,\n",
    "        download=True,\n",
    "        transform=transform_test\n",
    "    )\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "    # Create model\n",
    "    model = CustomTwoBranchViT(\n",
    "        img_size=224,\n",
    "        embed_dim=256,     # smaller than typical ViT-Base for demonstration\n",
    "        num_heads=4,\n",
    "        mlp_ratio=4.0,\n",
    "        num_classes=10,\n",
    "        dropout=0.1\n",
    "    ).to(device)\n",
    "\n",
    "    # Optimizer / Loss\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Training loop\n",
    "    best_acc = 0.0\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        epoch_loss = running_loss / len(train_loader)\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "        # Evaluate\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in test_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = model(images)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                correct += (preds == labels).sum().item()\n",
    "                total += labels.size(0)\n",
    "\n",
    "        acc = 100.0 * correct / total\n",
    "        print(f\"Test Accuracy: {acc:.2f}%\")\n",
    "\n",
    "        if acc > best_acc:\n",
    "            best_acc = acc\n",
    "            torch.save(model.state_dict(), \"best_two_branch_vit_cifar10.pth\")\n",
    "            print(\"  [*] Best model saved.\")\n",
    "\n",
    "    print(f\"Training completed. Best accuracy: {best_acc:.2f}%\")\n",
    "\n",
    "train_on_cifar10()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cu12.4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
