{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1563 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Paths to the Tiny ImageNet dataset\n",
    "TRAIN_DIR = 'tiny-imagenet-200/train'\n",
    "VAL_DIR = 'tiny-imagenet-200/val'\n",
    "\n",
    "# Hyperparameters\n",
    "BATCH_SIZE = 64\n",
    "NUM_EPOCHS = 20\n",
    "LEARNING_RATE = 0.001\n",
    "NUM_CLASSES = 200\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "class TinyImageNetDataset(Dataset):\n",
    "    def __init__(self, data, labels, transform=None):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "        self.le = LabelEncoder()\n",
    "        self.le.fit(labels)\n",
    "        self.encoded_labels = self.le.transform(labels)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.data[idx]\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        label = self.encoded_labels[idx]\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, label\n",
    "\n",
    "def load_training_data():\n",
    "    data = []\n",
    "    labels = []\n",
    "    for wnid in os.listdir(TRAIN_DIR):\n",
    "        wnid_dir = os.path.join(TRAIN_DIR, wnid, 'images')\n",
    "        if os.path.isdir(wnid_dir):\n",
    "            for img_file in os.listdir(wnid_dir):\n",
    "                img_path = os.path.join(wnid_dir, img_file)\n",
    "                data.append(img_path)\n",
    "                labels.append(wnid)\n",
    "    return data, labels\n",
    "\n",
    "def load_validation_data():\n",
    "    val_data = []\n",
    "    val_labels = []\n",
    "    annotations_file = os.path.join(VAL_DIR, 'val_annotations.txt')\n",
    "    val_annotations = {}\n",
    "    with open(annotations_file, 'r') as f:\n",
    "        for line in f:\n",
    "            tokens = line.strip().split('\\t')\n",
    "            filename, wnid = tokens[0], tokens[1]\n",
    "            val_annotations[filename] = wnid\n",
    "    images_dir = os.path.join(VAL_DIR, 'images')\n",
    "    for img_file in os.listdir(images_dir):\n",
    "        img_path = os.path.join(images_dir, img_file)\n",
    "        if img_file in val_annotations:\n",
    "            val_data.append(img_path)\n",
    "            val_labels.append(val_annotations[img_file])\n",
    "    return val_data, val_labels\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((64, 64)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.480, 0.448, 0.398],\n",
    "                         std=[0.277, 0.269, 0.282])\n",
    "])\n",
    "\n",
    "# Load training data\n",
    "train_data, train_labels = load_training_data()\n",
    "train_dataset = TinyImageNetDataset(train_data, train_labels, transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n",
    "\n",
    "# Load validation data\n",
    "val_data, val_labels = load_validation_data()\n",
    "\n",
    "# Split validation data into new validation and test sets\n",
    "val_data, test_data, val_labels, test_labels = train_test_split(\n",
    "    val_data, val_labels, test_size=0.5, random_state=42, stratify=val_labels)\n",
    "\n",
    "# Create validation dataset and dataloader\n",
    "val_dataset = TinyImageNetDataset(val_data, val_labels, transform=transform)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
    "\n",
    "# Create test dataset and dataloader\n",
    "test_dataset = TinyImageNetDataset(test_data, test_labels, transform=transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
    "\n",
    "# Custom Multi-Head Attention with learnable alpha and beta\n",
    "class CustomMultiheadAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, dropout=0.1):\n",
    "        super(CustomMultiheadAttention, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.dropout = dropout\n",
    "\n",
    "        assert embed_dim % num_heads == 0, \"Embedding dimension must be divisible by num_heads.\"\n",
    "\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        self.scale = self.head_dim ** -0.5\n",
    "\n",
    "        # Projection layers\n",
    "        self.q_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.k_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.v_proj = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "        # Output projection\n",
    "        self.out_proj = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "        # Learnable parameters for combining attentions\n",
    "        self.gamma = nn.Parameter(torch.tensor([0.5, 0.5]))  # [alpha, beta]\n",
    "\n",
    "    def forward(self, query, key, value, num_patches1):\n",
    "        B, N, C = query.shape  # Batch size, sequence length, embedding dimension\n",
    "\n",
    "        # Linear projections\n",
    "        Q = self.q_proj(query)\n",
    "        K = self.k_proj(key)\n",
    "        V = self.v_proj(value)\n",
    "\n",
    "        # Reshape for multi-head attention\n",
    "        Q = Q.view(B, N, self.num_heads, self.head_dim).transpose(1, 2)  # (B, num_heads, N, head_dim)\n",
    "        K = K.view(B, N, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        V = V.view(B, N, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        # Compute attention scores\n",
    "        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) * self.scale  # (B, num_heads, N, N)\n",
    "\n",
    "        # Apply softmax to get attention weights\n",
    "        attn_weights = F.softmax(attn_scores, dim=-1)  # (B, num_heads, N, N)\n",
    "        attn_weights = F.dropout(attn_weights, p=self.dropout, training=self.training)\n",
    "\n",
    "        # Apply alpha and beta to attention weights before multiplying with V\n",
    "        gamma = torch.softmax(self.gamma, dim=0)  # Ensure alpha and beta sum to 1\n",
    "        alpha, beta = gamma[0], gamma[1]\n",
    "\n",
    "        # Adjusted for CLS token\n",
    "        num_tokens = attn_weights.size(-1)\n",
    "        cls_attn_weights = attn_weights[:, :, :, 0:1]  # CLS token attention weights\n",
    "        patch_attn_weights = attn_weights[:, :, :, 1:]  # Exclude CLS token\n",
    "\n",
    "        # Split attention weights and V according to the number of patches\n",
    "        attn_weights1 = patch_attn_weights[:, :, :, :num_patches1] * alpha\n",
    "        attn_weights2 = patch_attn_weights[:, :, :, num_patches1:] * beta\n",
    "\n",
    "        V1 = V[:, :, 1:num_patches1+1, :]\n",
    "        V2 = V[:, :, num_patches1+1:, :]\n",
    "\n",
    "        # Compute attention outputs\n",
    "        attn_output1 = torch.matmul(attn_weights1, V1)\n",
    "        attn_output2 = torch.matmul(attn_weights2, V2)\n",
    "\n",
    "        # Concatenate CLS token attention outputs\n",
    "        attn_output_cls = torch.matmul(cls_attn_weights, V[:, :, :1, :])  # CLS token\n",
    "        attn_output = torch.cat([attn_output_cls, attn_output1, attn_output2], dim=2)  # (B, num_heads, N, head_dim)\n",
    "        attn_output = attn_output.transpose(1, 2).reshape(B, N, self.embed_dim)\n",
    "\n",
    "        # Final linear projection\n",
    "        output = self.out_proj(attn_output)\n",
    "\n",
    "        return output\n",
    "\n",
    "# Custom Transformer Encoder Layer using CustomMultiheadAttention\n",
    "class CustomTransformerEncoderLayer(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, dim_feedforward=2048, dropout=0.1):\n",
    "        super(CustomTransformerEncoderLayer, self).__init__()\n",
    "        self.self_attn = CustomMultiheadAttention(embed_dim, num_heads, dropout=dropout)\n",
    "        self.linear1 = nn.Linear(embed_dim, dim_feedforward)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear2 = nn.Linear(dim_feedforward, embed_dim)\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "        self.activation = F.relu  # You can choose a different activation function if desired\n",
    "\n",
    "    def forward(self, src, num_patches1):\n",
    "        src2 = self.self_attn(src, src, src, num_patches1)\n",
    "        src = src + self.dropout1(src2)\n",
    "        src = self.norm1(src)\n",
    "\n",
    "        src2 = self.linear2(self.dropout(self.activation(self.linear1(src))))\n",
    "        src = src + self.dropout2(src2)\n",
    "        src = self.norm2(src)\n",
    "\n",
    "        return src\n",
    "\n",
    "# ViT model\n",
    "class ViT(nn.Module):\n",
    "    def __init__(self, img_size=64, patch_size1=16, patch_size2=8, num_classes=NUM_CLASSES, dim=512, depth=6, heads=8, mlp_dim=1024):\n",
    "        super(ViT, self).__init__()\n",
    "\n",
    "        # For patch size 16x16\n",
    "        num_patches1 = (img_size // patch_size1) ** 2  # 16 patches\n",
    "        patch_dim1 = 3 * patch_size1 * patch_size1\n",
    "        self.patch_size1 = patch_size1\n",
    "\n",
    "        # For patch size 8x8\n",
    "        num_patches2 = (img_size // patch_size2) ** 2  # 64 patches\n",
    "        patch_dim2 = 3 * patch_size2 * patch_size2\n",
    "        self.patch_size2 = patch_size2\n",
    "\n",
    "        # Total number of patches after processing\n",
    "        self.num_patches1 = num_patches1 * 4  # After repeating each 16x16 patch 4 times\n",
    "        self.num_patches2 = num_patches2\n",
    "        total_patches = self.num_patches1 + self.num_patches2\n",
    "\n",
    "        # Linear projection layers\n",
    "        self.to_patch_embedding1 = nn.Linear(patch_dim1, dim)\n",
    "        self.to_patch_embedding2 = nn.Linear(patch_dim2, dim)\n",
    "\n",
    "        # Positional embeddings\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, total_patches + 1, dim))  # +1 for CLS token\n",
    "\n",
    "        # CLS token\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, dim))\n",
    "\n",
    "        # Transformer Encoder\n",
    "        encoder_layer = CustomTransformerEncoderLayer(embed_dim=dim, num_heads=heads, dim_feedforward=mlp_dim)\n",
    "        self.transformer = nn.ModuleList([encoder_layer for _ in range(depth)])\n",
    "\n",
    "        # Classification head\n",
    "        self.mlp_head = nn.Sequential(\n",
    "            nn.LayerNorm(dim),\n",
    "            nn.Linear(dim, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "\n",
    "        # First Pass: Patches of size 16x16\n",
    "        patches1 = x.unfold(2, self.patch_size1, self.patch_size1).unfold(3, self.patch_size1, self.patch_size1)\n",
    "        patches1 = patches1.contiguous().view(B, C, -1, self.patch_size1, self.patch_size1)\n",
    "        patches1 = patches1.permute(0, 2, 1, 3, 4)\n",
    "        patches1 = patches1.reshape(B, -1, 3 * self.patch_size1 * self.patch_size1)\n",
    "\n",
    "        # Repeat each patch 4 times\n",
    "        patches1 = patches1.unsqueeze(2).repeat(1, 1, 4, 1).reshape(B, -1, 3 * self.patch_size1 * self.patch_size1)\n",
    "\n",
    "        # Embedding\n",
    "        tokens1 = self.to_patch_embedding1(patches1)\n",
    "\n",
    "        # Second Pass: Patches of size 8x8 with custom order\n",
    "        patches2 = x.unfold(2, self.patch_size2, self.patch_size2).unfold(3, self.patch_size2, self.patch_size2)\n",
    "        patches2 = patches2.contiguous().view(B, C, -1, self.patch_size2, self.patch_size2)\n",
    "        patches2 = patches2.permute(0, 2, 1, 3, 4)\n",
    "        patches2 = patches2.reshape(B, -1, 3 * self.patch_size2 * self.patch_size2)\n",
    "\n",
    "        # Reordering patches\n",
    "        order = [0,1,8,9,2,3,10,11,4,5,12,13,6,7,14,15,\n",
    "                 16,17,24,25,18,19,26,27,20,21,28,29,22,23,30,31,\n",
    "                 32,33,40,41,34,35,42,43,36,37,44,45,38,39,46,47,\n",
    "                 48,49,56,57,50,51,58,59,52,53,60,61,54,55,62,63]\n",
    "        order = torch.tensor(order).to(x.device)\n",
    "        patches2 = patches2[:, order, :]\n",
    "\n",
    "        # Embedding\n",
    "        tokens2 = self.to_patch_embedding2(patches2)\n",
    "\n",
    "        # Combine tokens\n",
    "        tokens = torch.cat((tokens1, tokens2), dim=1)  # (B, total_patches, dim)\n",
    "\n",
    "        # Add CLS token\n",
    "        cls_tokens = self.cls_token.expand(B, -1, -1)  # (B, 1, dim)\n",
    "        tokens = torch.cat((cls_tokens, tokens), dim=1)  # (B, total_patches + 1, dim)\n",
    "\n",
    "        # Add positional embeddings\n",
    "        tokens += self.pos_embedding[:, :tokens.size(1), :]\n",
    "\n",
    "        # Transformer encoding\n",
    "        for encoder_layer in self.transformer:\n",
    "            tokens = encoder_layer(tokens, self.num_patches1)\n",
    "\n",
    "        # Classification using CLS token\n",
    "        cls_token_final = tokens[:, 0, :]  # (B, dim)\n",
    "        out = self.mlp_head(cls_token_final)\n",
    "\n",
    "        return out\n",
    "\n",
    "def accuracy(output, target, topk=(1, 3, 5)):\n",
    "    \"\"\"Computes the accuracy over the k top predictions\"\"\"\n",
    "    maxk = max(topk)\n",
    "    batch_size = target.size(0)\n",
    "\n",
    "    _, pred = output.topk(maxk, 1, True, True)\n",
    "    pred = pred.t()\n",
    "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "    res = []\n",
    "    for k in topk:\n",
    "        correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n",
    "        res.append(correct_k.mul_(100.0 / batch_size))\n",
    "    return res\n",
    "\n",
    "def train(model, device, train_loader, criterion, optimizer, epoch):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    top1_acc = 0.0\n",
    "    top3_acc = 0.0\n",
    "    top5_acc = 0.0\n",
    "    total = 0\n",
    "\n",
    "    loop = tqdm(train_loader)\n",
    "    for batch_idx, (inputs, targets) in enumerate(loop):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        acc1, acc3, acc5 = accuracy(outputs, targets, topk=(1, 3, 5))\n",
    "        top1_acc += acc1.item()\n",
    "        top3_acc += acc3.item()\n",
    "        top5_acc += acc5.item()\n",
    "        total += 1\n",
    "\n",
    "        # Update tqdm loop\n",
    "        loop.set_description(f\"Epoch [{epoch}/{NUM_EPOCHS}]\")\n",
    "        loop.set_postfix(loss=(running_loss/total), top1_acc=(top1_acc/total), top3_acc=(top3_acc/total), top5_acc=(top5_acc/total))\n",
    "\n",
    "    return running_loss/total, top1_acc/total, top3_acc/total, top5_acc/total\n",
    "\n",
    "def validate(model, device, val_loader, criterion):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    top1_acc = 0.0\n",
    "    top3_acc = 0.0\n",
    "    top5_acc = 0.0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        loop = tqdm(val_loader)\n",
    "        for batch_idx, (inputs, targets) in enumerate(loop):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            acc1, acc3, acc5 = accuracy(outputs, targets, topk=(1, 3, 5))\n",
    "            top1_acc += acc1.item()\n",
    "            top3_acc += acc3.item()\n",
    "            top5_acc += acc5.item()\n",
    "            total += 1\n",
    "\n",
    "            # Update tqdm loop\n",
    "            loop.set_description(f\"Validation\")\n",
    "            loop.set_postfix(loss=(running_loss/total), top1_acc=(top1_acc/total), top3_acc=(top3_acc/total), top5_acc=(top5_acc/total))\n",
    "\n",
    "    return running_loss/total, top1_acc/total, top3_acc/total, top5_acc/total\n",
    "\n",
    "def test(model, device, test_loader, criterion):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    top1_acc = 0.0\n",
    "    top3_acc = 0.0\n",
    "    top5_acc = 0.0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        loop = tqdm(test_loader)\n",
    "        for batch_idx, (inputs, targets) in enumerate(loop):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            acc1, acc3, acc5 = accuracy(outputs, targets, topk=(1, 3, 5))\n",
    "            top1_acc += acc1.item()\n",
    "            top3_acc += acc3.item()\n",
    "            top5_acc += acc5.item()\n",
    "            total += 1\n",
    "\n",
    "            # Update tqdm loop\n",
    "            loop.set_description(f\"Test\")\n",
    "            loop.set_postfix(loss=(running_loss/total), top1_acc=(top1_acc/total), top3_acc=(top3_acc/total), top5_acc=(top5_acc/total))\n",
    "\n",
    "    avg_loss = running_loss / total\n",
    "    avg_top1_acc = top1_acc / total\n",
    "    avg_top3_acc = top3_acc / total\n",
    "    avg_top5_acc = top5_acc / total\n",
    "\n",
    "    print(f\"Test Loss: {avg_loss:.4f}\")\n",
    "    print(f\"Test Acc@1: {avg_top1_acc:.2f}%\")\n",
    "    print(f\"Test Acc@3: {avg_top3_acc:.2f}%\")\n",
    "    print(f\"Test Acc@5: {avg_top5_acc:.2f}%\")\n",
    "\n",
    "    return avg_loss, avg_top1_acc, avg_top3_acc, avg_top5_acc\n",
    "\n",
    "model = ViT().to(DEVICE)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "train_top1_acc = []\n",
    "val_top1_acc = []\n",
    "train_top3_acc = []\n",
    "val_top3_acc = []\n",
    "train_top5_acc = []\n",
    "val_top5_acc = []\n",
    "\n",
    "best_acc = 0.0\n",
    "\n",
    "for epoch in range(1, NUM_EPOCHS + 1):\n",
    "    train_loss, train_acc1, train_acc3, train_acc5 = train(model, DEVICE, train_loader, criterion, optimizer, epoch)\n",
    "    val_loss, val_acc1, val_acc3, val_acc5 = validate(model, DEVICE, val_loader, criterion)\n",
    "\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    train_top1_acc.append(train_acc1)\n",
    "    val_top1_acc.append(val_acc1)\n",
    "    train_top3_acc.append(train_acc3)\n",
    "    val_top3_acc.append(val_acc3)\n",
    "    train_top5_acc.append(train_acc5)\n",
    "    val_top5_acc.append(val_acc5)\n",
    "\n",
    "    # Save the best model\n",
    "    if val_acc1 > best_acc:\n",
    "        best_acc = val_acc1\n",
    "        torch.save(model.state_dict(), 'best_model.pth')\n",
    "        print(f\"Best model saved with accuracy: {best_acc:.2f}%\")\n",
    "\n",
    "    print(f\"Epoch {epoch}: Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "    print(f\"Train Acc@1: {train_acc1:.2f}%, Val Acc@1: {val_acc1:.2f}%\")\n",
    "    print(f\"Train Acc@3: {train_acc3:.2f}%, Val Acc@3: {val_acc3:.2f}%\")\n",
    "    print(f\"Train Acc@5: {train_acc5:.2f}%, Val Acc@5: {val_acc5:.2f}%\")\n",
    "\n",
    "def plot_metrics(train_metric, val_metric, metric_name):\n",
    "    epochs = range(1, NUM_EPOCHS + 1)\n",
    "    plt.figure()\n",
    "    plt.plot(epochs, train_metric, 'b', label=f'Training {metric_name}')\n",
    "    plt.plot(epochs, val_metric, 'r', label=f'Validation {metric_name}')\n",
    "    plt.title(f'Training and Validation {metric_name}')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel(metric_name)\n",
    "    plt.legend()\n",
    "    plt.savefig(f'{metric_name}.png')\n",
    "    plt.show()\n",
    "\n",
    "# Plot Loss\n",
    "plot_metrics(train_losses, val_losses, 'Loss')\n",
    "\n",
    "# Plot Top-1 Accuracy\n",
    "plot_metrics(train_top1_acc, val_top1_acc, 'Top-1 Accuracy')\n",
    "\n",
    "# Plot Top-3 Accuracy\n",
    "plot_metrics(train_top3_acc, val_top3_acc, 'Top-3 Accuracy')\n",
    "\n",
    "# Plot Top-5 Accuracy\n",
    "plot_metrics(train_top5_acc, val_top5_acc, 'Top-5 Accuracy')\n",
    "\n",
    "print(\"Model Parameters:\")\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(f\"{name}: {param.data.shape}\")\n",
    "\n",
    "# Load the best model\n",
    "model.load_state_dict(torch.load('best_model.pth'))\n",
    "\n",
    "# Evaluate on test set\n",
    "test_loss, test_acc1, test_acc3, test_acc5 = test(model, DEVICE, test_loader, criterion)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cu12.4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
