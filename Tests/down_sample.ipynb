{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data\\cifar-10-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170498071/170498071 [02:12<00:00, 1286529.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\cifar-10-python.tar.gz to ./data\n",
      "Files already downloaded and verified\n",
      "Model has 89.86 M learnable parameters.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 348\u001b[0m\n\u001b[0;32m    346\u001b[0m epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m\n\u001b[0;32m    347\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, epochs\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m--> 348\u001b[0m     train_loss, train_top1, train_top5 \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    349\u001b[0m     val_loss, val_top1, val_top5 \u001b[38;5;241m=\u001b[39m evaluate(model, val_loader, device)\n\u001b[0;32m    351\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[1], line 269\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[1;34m(model, loader, optimizer, device)\u001b[0m\n\u001b[0;32m    266\u001b[0m labels \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m    268\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m--> 269\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    270\u001b[0m loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mcross_entropy(logits, labels)\n\u001b[0;32m    271\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[1], line 213\u001b[0m, in \u001b[0;36mProgressiveTransformer.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    211\u001b[0m \u001b[38;5;66;03m# 2) Pass through 12 blocks, downsampling after 3,6,9\u001b[39;00m\n\u001b[0;32m    212\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks, start\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m--> 213\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# (B, N, 786)\u001b[39;00m\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m6\u001b[39m, \u001b[38;5;241m9\u001b[39m]:  \u001b[38;5;66;03m# downsample after these blocks\u001b[39;00m\n\u001b[0;32m    216\u001b[0m         x \u001b[38;5;241m=\u001b[39m pairwise_mean_downsample(x) \n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[1], line 130\u001b[0m, in \u001b[0;36mTransformerEncoderBlock.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    127\u001b[0m attn_out, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattn(x_norm, x_norm, x_norm)  \u001b[38;5;66;03m# shape (B, N, hidden_dim)\u001b[39;00m\n\u001b[0;32m    128\u001b[0m x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m attn_out\n\u001b[1;32m--> 130\u001b[0m x_norm \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mln2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    131\u001b[0m x_mlp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp(x_norm)\n\u001b[0;32m    132\u001b[0m x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m x_mlp\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\normalization.py:201\u001b[0m, in \u001b[0;36mLayerNorm.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    200\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 201\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    202\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalized_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\functional.py:2546\u001b[0m, in \u001b[0;36mlayer_norm\u001b[1;34m(input, normalized_shape, weight, bias, eps)\u001b[0m\n\u001b[0;32m   2542\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_variadic(\u001b[38;5;28minput\u001b[39m, weight, bias):\n\u001b[0;32m   2543\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m   2544\u001b[0m         layer_norm, (\u001b[38;5;28minput\u001b[39m, weight, bias), \u001b[38;5;28minput\u001b[39m, normalized_shape, weight\u001b[38;5;241m=\u001b[39mweight, bias\u001b[38;5;241m=\u001b[39mbias, eps\u001b[38;5;241m=\u001b[39meps\n\u001b[0;32m   2545\u001b[0m     )\n\u001b[1;32m-> 2546\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer_norm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnormalized_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackends\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcudnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menabled\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "import math\n",
    "\n",
    "# -------------------------\n",
    "# 1. CIFAR-10 Datasets & Dataloaders\n",
    "# -------------------------\n",
    "def get_cifar10_loaders(batch_size=64, num_workers=2):\n",
    "    \"\"\"\n",
    "    Returns train/val loaders for CIFAR-10 dataset.\n",
    "    Images are resized from 32x32 to 224x224 and normalized.\n",
    "    \"\"\"\n",
    "    # Common CIFAR-10 statistics\n",
    "    mean = (0.4914, 0.4822, 0.4465)\n",
    "    std  = (0.2470, 0.2435, 0.2616)\n",
    "\n",
    "    # Train transforms: resize to 224, random crop, random flip\n",
    "    train_transform = T.Compose([\n",
    "        T.Resize(224),\n",
    "        T.RandomCrop(224, padding=4),\n",
    "        T.RandomHorizontalFlip(),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(mean, std),\n",
    "    ])\n",
    "\n",
    "    # Validation transforms: resize to 224, just center crop or direct resize\n",
    "    val_transform = T.Compose([\n",
    "        T.Resize(224),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(mean, std),\n",
    "    ])\n",
    "\n",
    "    # Download CIFAR-10\n",
    "    train_dataset = torchvision.datasets.CIFAR10(\n",
    "        root='./data',\n",
    "        train=True,\n",
    "        download=True,\n",
    "        transform=train_transform\n",
    "    )\n",
    "    val_dataset = torchvision.datasets.CIFAR10(\n",
    "        root='./data',\n",
    "        train=False,\n",
    "        download=True,\n",
    "        transform=val_transform\n",
    "    )\n",
    "\n",
    "    # Create Data Loaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size,\n",
    "                              shuffle=True, num_workers=num_workers)\n",
    "    val_loader   = DataLoader(val_dataset,   batch_size=batch_size,\n",
    "                              shuffle=False, num_workers=num_workers)\n",
    "    return train_loader, val_loader\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# 2. Progressive Transformer Model\n",
    "# -------------------------\n",
    "\n",
    "class PatchEmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "    Splits 224x224 images into (224/8=28) x (224/8=28)=784 patches,\n",
    "    each of size 8x8x3 flattened -> projected to hidden_dim.\n",
    "    Adds a [CLS] token and learns positional embeddings.\n",
    "    \"\"\"\n",
    "    def __init__(self, img_size=224, patch_size=8, in_chans=3, hidden_dim=786):\n",
    "        super().__init__()\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.grid_size = img_size // patch_size  # 224/8=28\n",
    "        num_patches = self.grid_size * self.grid_size  # 784\n",
    "\n",
    "        self.proj = nn.Conv2d(in_chans, hidden_dim,\n",
    "                              kernel_size=patch_size, stride=patch_size)\n",
    "        # This yields shape: (B, hidden_dim, 28, 28) => flatten => (B, 784, hidden_dim)\n",
    "\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, hidden_dim))\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches+1, hidden_dim))\n",
    "\n",
    "        self.num_patches = num_patches\n",
    "\n",
    "    def forward(self, x):\n",
    "        B = x.shape[0]\n",
    "        # x: (B, 3, 224, 224)\n",
    "        x = self.proj(x)  # -> (B, hidden_dim, 28, 28)\n",
    "        x = x.flatten(2)  # -> (B, hidden_dim, 784)\n",
    "        x = x.transpose(1, 2)  # -> (B, 784, hidden_dim)\n",
    "\n",
    "        # Concat CLS token\n",
    "        cls_token = self.cls_token.expand(B, -1, -1)  # (B, 1, hidden_dim)\n",
    "        x = torch.cat([cls_token, x], dim=1)  # (B, 785, hidden_dim)\n",
    "\n",
    "        # Add positional embeddings\n",
    "        x = x + self.pos_embed[:, : x.size(1), :]  # broadcast along batch\n",
    "\n",
    "        return x  # (B, 785, hidden_dim)\n",
    "\n",
    "\n",
    "class TransformerEncoderBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Standard Transformer block: MHSA + MLP (with skip & layernorm).\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_dim=786, num_heads=6, mlp_ratio=4.0, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(hidden_dim)\n",
    "        self.attn = nn.MultiheadAttention(embed_dim=hidden_dim,\n",
    "                                          num_heads=num_heads,\n",
    "                                          dropout=dropout,\n",
    "                                          batch_first=True)  # batch_first => (B, N, C)\n",
    "\n",
    "        self.ln2 = nn.LayerNorm(hidden_dim)\n",
    "\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, int(hidden_dim * mlp_ratio)),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(int(hidden_dim * mlp_ratio), hidden_dim),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (B, N, hidden_dim)\n",
    "        x_norm = self.ln1(x)\n",
    "        attn_out, _ = self.attn(x_norm, x_norm, x_norm)  # shape (B, N, hidden_dim)\n",
    "        x = x + attn_out\n",
    "\n",
    "        x_norm = self.ln2(x)\n",
    "        x_mlp = self.mlp(x_norm)\n",
    "        x = x + x_mlp\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "def pairwise_mean_downsample(x):\n",
    "    \"\"\"\n",
    "    x shape: (B, N, C)\n",
    "    We'll keep the first token (CLS) separate,\n",
    "    then take pairs of patch tokens, averaging them.\n",
    "\n",
    "    Example:\n",
    "      - CLS index = 0 => remains alone\n",
    "      - Patch tokens = x[:, 1:, :] => shape (B, N-1, C)\n",
    "\n",
    "      We'll pair (0,1), (2,3), ...\n",
    "      So the new patch token count = (N-1)//2\n",
    "\n",
    "      Then we re-concat CLS in front => total tokens = (N-1)//2 + 1\n",
    "    \"\"\"\n",
    "    B, N, C = x.shape\n",
    "    assert N > 1, \"We need patch tokens + CLS at least.\"\n",
    "\n",
    "    cls_token = x[:, 0:1, :]    # (B, 1, C)\n",
    "    patch_tokens = x[:, 1:, :]  # (B, N-1, C)\n",
    "\n",
    "    # Pairwise reshape: (B, (N-1)//2, 2, C) and take mean along dim=2\n",
    "    # But we must ensure (N-1) is even, as described in your scenario\n",
    "    # e.g., 784 -> 392, 392 -> 196, etc.\n",
    "    assert (patch_tokens.shape[1] % 2) == 0, \\\n",
    "        f\"Number of patch tokens must be even, got {patch_tokens.shape[1]}\"\n",
    "\n",
    "    patch_tokens = patch_tokens.reshape(B, patch_tokens.shape[1] // 2, 2, C)\n",
    "    patch_tokens = patch_tokens.mean(dim=2)  # (B, new_N, C)\n",
    "\n",
    "    # Concat CLS\n",
    "    x_down = torch.cat([cls_token, patch_tokens], dim=1)\n",
    "    return x_down\n",
    "\n",
    "\n",
    "class ProgressiveTransformer(nn.Module):\n",
    "    \"\"\"\n",
    "    A 12-layer Transformer with hidden_dim=786, that:\n",
    "      - uses patch size=8 => 785 tokens initially\n",
    "      - after block 3: downsample 784 patch -> 392 patch => total 393\n",
    "      - after block 6: downsample 392 -> 196 => total 197\n",
    "      - after block 9: downsample 196 -> 98 => total 99\n",
    "      - blocks 10, 11, 12 keep it at 99 tokens\n",
    "      - final linear for 10-class CIFAR output\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 img_size=224,\n",
    "                 patch_size=8,\n",
    "                 in_chans=3,\n",
    "                 hidden_dim=786,\n",
    "                 num_heads=6,       # 6 * 131 = 786\n",
    "                 mlp_ratio=4.0,\n",
    "                 num_layers=12,\n",
    "                 num_classes=10,\n",
    "                 dropout=0.0):\n",
    "        super().__init__()\n",
    "\n",
    "        # Patch embedding\n",
    "        self.patch_embed = PatchEmbedding(img_size, patch_size, in_chans, hidden_dim)\n",
    "\n",
    "        # Create 12 Transformer blocks\n",
    "        self.blocks = nn.ModuleList([\n",
    "            TransformerEncoderBlock(hidden_dim, num_heads, mlp_ratio, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "        # Final norm + classifier\n",
    "        self.norm = nn.LayerNorm(hidden_dim)\n",
    "        self.head = nn.Linear(hidden_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 1) Patchify + Embeddings\n",
    "        x = self.patch_embed(x)  # (B, 785, 786) initially\n",
    "\n",
    "        # 2) Pass through 12 blocks, downsampling after 3,6,9\n",
    "        for i, block in enumerate(self.blocks, start=1):\n",
    "            x = block(x)  # (B, N, 786)\n",
    "            \n",
    "            if i in [3, 6, 9]:  # downsample after these blocks\n",
    "                x = pairwise_mean_downsample(x) \n",
    "                # e.g. after block 3: (B, 393, 786)\n",
    "                #     after block 6: (B, 197, 786)\n",
    "                #     after block 9: (B, 99, 786)\n",
    "\n",
    "        # 3) Final norm + CLS for classification\n",
    "        x = self.norm(x)  # (B, N, 786)\n",
    "        cls_token = x[:, 0]  # (B, 786)\n",
    "        out = self.head(cls_token)  # (B, 10)\n",
    "        return out\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# 3. Training / Evaluation Helpers\n",
    "# -------------------------\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "def accuracy_topk(logits, targets, topk=(1,5)):\n",
    "    \"\"\"\n",
    "    Computes top-k accuracy for specified k values.\n",
    "    Returns list of accuracies in percentage.\n",
    "    \"\"\"\n",
    "    max_k = max(topk)\n",
    "    batch_size = targets.size(0)\n",
    "\n",
    "    # Get top-k predictions\n",
    "    _, pred = logits.topk(max_k, dim=1, largest=True, sorted=True)\n",
    "    pred = pred.t()  # shape (max_k, B)\n",
    "    correct = pred.eq(targets.view(1, -1).expand_as(pred))\n",
    "\n",
    "    res = []\n",
    "    for k in topk:\n",
    "        # For each k, compute how many of the batch are correct in top-k\n",
    "        correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n",
    "        acc_k = correct_k * 100.0 / batch_size\n",
    "        res.append(acc_k.item())\n",
    "    return res\n",
    "\n",
    "\n",
    "def train_one_epoch(model, loader, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    total_samples = 0\n",
    "    top1_sum = 0.0\n",
    "    top5_sum = 0.0\n",
    "\n",
    "    for images, labels in loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(images)\n",
    "        loss = F.cross_entropy(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # stats\n",
    "        batch_size = images.size(0)\n",
    "        total_loss += loss.item() * batch_size\n",
    "        total_samples += batch_size\n",
    "\n",
    "        acc1, acc5 = accuracy_topk(logits, labels, topk=(1,5))\n",
    "        top1_sum += acc1 * batch_size\n",
    "        top5_sum += acc5 * batch_size\n",
    "\n",
    "    avg_loss = total_loss / total_samples\n",
    "    avg_top1 = top1_sum / total_samples\n",
    "    avg_top5 = top5_sum / total_samples\n",
    "\n",
    "    return avg_loss, avg_top1, avg_top5\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader, device):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_samples = 0\n",
    "    top1_sum = 0.0\n",
    "    top5_sum = 0.0\n",
    "\n",
    "    for images, labels in loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        logits = model(images)\n",
    "        loss = F.cross_entropy(logits, labels)\n",
    "\n",
    "        batch_size = images.size(0)\n",
    "        total_loss += loss.item() * batch_size\n",
    "        total_samples += batch_size\n",
    "\n",
    "        acc1, acc5 = accuracy_topk(logits, labels, topk=(1,5))\n",
    "        top1_sum += acc1 * batch_size\n",
    "        top5_sum += acc5 * batch_size\n",
    "\n",
    "    avg_loss = total_loss / total_samples\n",
    "    avg_top1 = top1_sum / total_samples\n",
    "    avg_top5 = top5_sum / total_samples\n",
    "\n",
    "    return avg_loss, avg_top1, avg_top5\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# 4. Putting It All Together (Example Training Script)\n",
    "# -------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    train_loader, val_loader = get_cifar10_loaders(batch_size=32)\n",
    "\n",
    "    model = ProgressiveTransformer(\n",
    "        img_size=224,\n",
    "        patch_size=8,\n",
    "        hidden_dim=786,   # user-specified\n",
    "        num_heads=6,      # 6 heads x 131 dim each = 786\n",
    "        mlp_ratio=4.0,\n",
    "        num_layers=12,\n",
    "        num_classes=10,\n",
    "        dropout=0.1\n",
    "    ).to(device)\n",
    "\n",
    "    # Number of parameters\n",
    "    num_params = count_parameters(model)\n",
    "    print(f\"Model has {num_params/1e6:.2f} M learnable parameters.\")\n",
    "\n",
    "    # Simple AdamW optimizer\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4, weight_decay=1e-4)\n",
    "\n",
    "    # Train for a few epochs (example)\n",
    "    epochs = 5\n",
    "    for epoch in range(1, epochs+1):\n",
    "        train_loss, train_top1, train_top5 = train_one_epoch(model, train_loader, optimizer, device)\n",
    "        val_loss, val_top1, val_top5 = evaluate(model, val_loader, device)\n",
    "\n",
    "        print(f\"Epoch [{epoch}/{epochs}]\")\n",
    "        print(f\"  Train Loss: {train_loss:.4f}, Top1: {train_top1:.2f}%, Top5: {train_top5:.2f}%\")\n",
    "        print(f\"  Val   Loss: {val_loss:.4f},   Top1: {val_top1:.2f}%,   Top5: {val_top5:.2f}%\")\n",
    "\n",
    "    print(\"Training complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "import math\n",
    "\n",
    "# -------------------------\n",
    "# 1. CIFAR-10 Datasets & Dataloaders\n",
    "# -------------------------\n",
    "def get_cifar10_loaders(batch_size=64, num_workers=2):\n",
    "    \"\"\"\n",
    "    Returns train/val loaders for CIFAR-10 dataset.\n",
    "    Images are resized from 32x32 to 224x224 and normalized.\n",
    "    \"\"\"\n",
    "    # CIFAR-10 mean/std\n",
    "    mean = (0.4914, 0.4822, 0.4465)\n",
    "    std  = (0.2470, 0.2435, 0.2616)\n",
    "\n",
    "    # Train transforms: resize to 224, random augmentations\n",
    "    train_transform = T.Compose([\n",
    "        T.Resize(224),\n",
    "        T.RandomCrop(224, padding=4),\n",
    "        T.RandomHorizontalFlip(),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(mean, std),\n",
    "    ])\n",
    "\n",
    "    # Validation transforms: resize to 224\n",
    "    val_transform = T.Compose([\n",
    "        T.Resize(224),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(mean, std),\n",
    "    ])\n",
    "\n",
    "    train_dataset = torchvision.datasets.CIFAR10(\n",
    "        root='./data',\n",
    "        train=True,\n",
    "        download=True,\n",
    "        transform=train_transform\n",
    "    )\n",
    "    val_dataset = torchvision.datasets.CIFAR10(\n",
    "        root='./data',\n",
    "        train=False,\n",
    "        download=True,\n",
    "        transform=val_transform\n",
    "    )\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size,\n",
    "                              shuffle=True, num_workers=num_workers)\n",
    "    val_loader   = DataLoader(val_dataset,   batch_size=batch_size,\n",
    "                              shuffle=False, num_workers=num_workers)\n",
    "    return train_loader, val_loader\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# 2. Model Components\n",
    "# -------------------------\n",
    "\n",
    "class PatchEmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "    Splits 224x224 images into (224/8=28) x (224/8=28)=784 patches,\n",
    "    each of size 8x8x3 flattened -> projected to hidden_dim=768.\n",
    "    Adds a [CLS] token and learns positional embeddings.\n",
    "    \"\"\"\n",
    "    def __init__(self, img_size=224, patch_size=8, in_chans=3, hidden_dim=768):\n",
    "        super().__init__()\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.grid_size = img_size // patch_size  # 224/8=28\n",
    "        num_patches = self.grid_size * self.grid_size  # 784\n",
    "\n",
    "        # Project each patch to hidden_dim\n",
    "        self.proj = nn.Conv2d(in_chans, hidden_dim,\n",
    "                              kernel_size=patch_size, stride=patch_size)\n",
    "        # -> shape: (B, 768, 28, 28) => flatten => (B, 784, 768)\n",
    "\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, hidden_dim))\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, hidden_dim))\n",
    "        self.num_patches = num_patches\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, 3, 224, 224)\n",
    "        B = x.shape[0]\n",
    "        x = self.proj(x)  # -> (B, 768, 28, 28)\n",
    "        x = x.flatten(2)  # -> (B, 768, 784)\n",
    "        x = x.transpose(1, 2)  # -> (B, 784, 768)\n",
    "\n",
    "        # Concat CLS token\n",
    "        cls_token = self.cls_token.expand(B, -1, -1)  # (B, 1, 768)\n",
    "        x = torch.cat([cls_token, x], dim=1)  # (B, 785, 768)\n",
    "\n",
    "        # Add positional embeddings\n",
    "        x = x + self.pos_embed[:, : x.size(1), :]\n",
    "        return x  # (B, 785, 768)\n",
    "\n",
    "\n",
    "class TransformerEncoderBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Standard Transformer block: MHSA + MLP (with skip connections & layernorm).\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_dim=768, num_heads=12, mlp_ratio=4.0, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(hidden_dim)\n",
    "        self.attn = nn.MultiheadAttention(embed_dim=hidden_dim,\n",
    "                                          num_heads=num_heads,\n",
    "                                          dropout=dropout,\n",
    "                                          batch_first=True)  # (B, N, C)\n",
    "        self.ln2 = nn.LayerNorm(hidden_dim)\n",
    "\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, int(hidden_dim * mlp_ratio)),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(int(hidden_dim * mlp_ratio), hidden_dim),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, N, 768)\n",
    "        x_norm = self.ln1(x)\n",
    "        attn_out, _ = self.attn(x_norm, x_norm, x_norm)  # (B, N, 768)\n",
    "        x = x + attn_out\n",
    "\n",
    "        x_norm = self.ln2(x)\n",
    "        mlp_out = self.mlp(x_norm)\n",
    "        x = x + mlp_out\n",
    "        return x\n",
    "\n",
    "\n",
    "def pairwise_mean_downsample(x):\n",
    "    \"\"\"\n",
    "    Downsamples the patch tokens by pairs:\n",
    "      - Keep CLS token separate (index 0).\n",
    "      - Pair up the patch tokens (indices 1...N-1) => average each pair.\n",
    "      - Ensure patch_tokens.shape[1] is even.\n",
    "    Output has roughly half the patch tokens + 1 CLS.\n",
    "    \"\"\"\n",
    "    B, N, C = x.shape\n",
    "    cls_token = x[:, :1, :]   # (B, 1, 768)\n",
    "    patch_tokens = x[:, 1:, :]  # (B, N-1, 768)\n",
    "\n",
    "    # Check even number of patch tokens\n",
    "    assert (patch_tokens.shape[1] % 2) == 0, \\\n",
    "        f\"Number of patch tokens must be even, got {patch_tokens.shape[1]}\"\n",
    "\n",
    "    # Reshape and average each pair\n",
    "    patch_tokens = patch_tokens.reshape(B, patch_tokens.shape[1] // 2, 2, C)\n",
    "    patch_tokens = patch_tokens.mean(dim=2)  # (B, new_N, 768)\n",
    "\n",
    "    # Concat CLS on the front\n",
    "    x_down = torch.cat([cls_token, patch_tokens], dim=1)  # (B, new_N+1, 768)\n",
    "    return x_down\n",
    "\n",
    "\n",
    "class ProgressiveTransformer(nn.Module):\n",
    "    \"\"\"\n",
    "    A 12-layer Transformer with hidden_dim=768.\n",
    "    - Patch size=8 -> 784 patches + 1 CLS = 785 tokens\n",
    "    - After blocks #3, #6, #9: pairwise-mean downsampling of patch tokens.\n",
    "    - Finally output a 10-class prediction for CIFAR-10.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 img_size=224,\n",
    "                 patch_size=8,\n",
    "                 in_chans=3,\n",
    "                 hidden_dim=768,\n",
    "                 num_heads=12,\n",
    "                 mlp_ratio=4.0,\n",
    "                 num_layers=12,\n",
    "                 num_classes=10,\n",
    "                 dropout=0.0):\n",
    "        super().__init__()\n",
    "\n",
    "        self.patch_embed = PatchEmbedding(img_size, patch_size, in_chans, hidden_dim)\n",
    "\n",
    "        self.blocks = nn.ModuleList([\n",
    "            TransformerEncoderBlock(hidden_dim, num_heads, mlp_ratio, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "        self.norm = nn.LayerNorm(hidden_dim)\n",
    "        self.head = nn.Linear(hidden_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 1) Embed patches + CLS\n",
    "        x = self.patch_embed(x)  # (B, 785, 768) initially\n",
    "\n",
    "        # 2) Pass through 12 blocks, downsampling after blocks 3, 6, 9\n",
    "        for i, block in enumerate(self.blocks, start=1):\n",
    "            x = block(x)\n",
    "            if i in [3, 6, 9]:\n",
    "                x = pairwise_mean_downsample(x)\n",
    "                # e.g. after block 3: from (B, 785, 768) -> (B, 393, 768)\n",
    "                #     after block 6: (B, 393, 768) -> (B, 197, 768)\n",
    "                #     after block 9: (B, 197, 768) -> (B, 99, 768)\n",
    "\n",
    "        # 3) Final norm + classifier\n",
    "        x = self.norm(x)         # (B, N, 768)\n",
    "        cls_token = x[:, 0]      # (B, 768)\n",
    "        out = self.head(cls_token)  # (B, 10)\n",
    "        return out\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# 3. Training / Evaluation Helpers\n",
    "# -------------------------\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "def accuracy_topk(logits, targets, topk=(1,5)):\n",
    "    \"\"\"\n",
    "    Computes top-k accuracy for specified k values.\n",
    "    Returns list of accuracies in percentage.\n",
    "    \"\"\"\n",
    "    max_k = max(topk)\n",
    "    batch_size = targets.size(0)\n",
    "\n",
    "    # Get top-k predictions\n",
    "    _, pred = logits.topk(max_k, dim=1, largest=True, sorted=True)\n",
    "    pred = pred.t()  # (max_k, B)\n",
    "    correct = pred.eq(targets.view(1, -1).expand_as(pred))\n",
    "\n",
    "    res = []\n",
    "    for k in topk:\n",
    "        correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n",
    "        acc_k = correct_k * 100.0 / batch_size\n",
    "        res.append(acc_k.item())\n",
    "    return res\n",
    "\n",
    "def train_one_epoch(model, loader, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    total_samples = 0\n",
    "    top1_sum = 0.0\n",
    "    top5_sum = 0.0\n",
    "\n",
    "    for images, labels in loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(images)\n",
    "        loss = F.cross_entropy(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        batch_size = images.size(0)\n",
    "        total_loss += loss.item() * batch_size\n",
    "        total_samples += batch_size\n",
    "\n",
    "        acc1, acc5 = accuracy_topk(logits, labels, topk=(1,5))\n",
    "        top1_sum += acc1 * batch_size\n",
    "        top5_sum += acc5 * batch_size\n",
    "\n",
    "    avg_loss = total_loss / total_samples\n",
    "    avg_top1 = top1_sum / total_samples\n",
    "    avg_top5 = top5_sum / total_samples\n",
    "    return avg_loss, avg_top1, avg_top5\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader, device):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_samples = 0\n",
    "    top1_sum = 0.0\n",
    "    top5_sum = 0.0\n",
    "\n",
    "    for images, labels in loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        logits = model(images)\n",
    "        loss = F.cross_entropy(logits, labels)\n",
    "\n",
    "        batch_size = images.size(0)\n",
    "        total_loss += loss.item() * batch_size\n",
    "        total_samples += batch_size\n",
    "\n",
    "        acc1, acc5 = accuracy_topk(logits, labels, topk=(1,5))\n",
    "        top1_sum += acc1 * batch_size\n",
    "        top5_sum += acc5 * batch_size\n",
    "\n",
    "    avg_loss = total_loss / total_samples\n",
    "    avg_top1 = top1_sum / total_samples\n",
    "    avg_top5 = top5_sum / total_samples\n",
    "    return avg_loss, avg_top1, avg_top5\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# 4. Putting It All Together (Example Training Script)\n",
    "# -------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    # Get CIFAR-10 data\n",
    "    train_loader, val_loader = get_cifar10_loaders(batch_size=32)\n",
    "\n",
    "    # Build Progressive Transformer (ViT-Base dimension=768, 12 heads, 12 layers)\n",
    "    model = ProgressiveTransformer(\n",
    "        img_size=224,\n",
    "        patch_size=8,\n",
    "        hidden_dim=768,\n",
    "        num_heads=12,      # 12 x 64 = 768\n",
    "        mlp_ratio=4.0,\n",
    "        num_layers=12,\n",
    "        num_classes=10,\n",
    "        dropout=0.1\n",
    "    ).to(device)\n",
    "\n",
    "    # Count parameters\n",
    "    num_params = count_parameters(model)\n",
    "    print(f\"Model has {num_params/1e6:.2f} M learnable parameters.\")\n",
    "\n",
    "    # Simple AdamW optimizer\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4, weight_decay=1e-4)\n",
    "\n",
    "    # Train for a few epochs as an example\n",
    "    epochs = 5\n",
    "    for epoch in range(1, epochs+1):\n",
    "        train_loss, train_top1, train_top5 = train_one_epoch(model, train_loader, optimizer, device)\n",
    "        val_loss, val_top1, val_top5 = evaluate(model, val_loader, device)\n",
    "\n",
    "        print(f\"Epoch [{epoch}/{epochs}]\")\n",
    "        print(f\"  Train Loss: {train_loss:.4f}, Top1: {train_top1:.2f}%, Top5: {train_top5:.2f}%\")\n",
    "        print(f\"  Val   Loss: {val_loss:.4f},   Top1: {val_top1:.2f}%,   Top5: {val_top5:.2f}%\")\n",
    "\n",
    "    print(\"Training complete.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cu12.4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
