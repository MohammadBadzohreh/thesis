{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "Total number of trainable parameters: 86223560\n",
      "\n",
      "Dummy output shape: torch.Size([2, 200])\n",
      "\n",
      "Epoch 1/20\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/1563 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append('..')\n",
    "\n",
    "from Utils.Accuracy_measures import topk_accuracy\n",
    "from Utils.TinyImageNet_loader import get_tinyimagenet_dataloaders\n",
    "from Utils.Num_parameter import count_parameters\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import time\n",
    "import torch\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import models, datasets, transforms\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set image size and batch size\n",
    "image_size = 224\n",
    "batch_size = 64\n",
    "\n",
    "# Define transforms for training, validation, and testing\n",
    "tiny_transform_train = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.Resize((image_size, image_size)),\n",
    "    transforms.RandomCrop(image_size, padding=5),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.485, 0.456, 0.406),\n",
    "                         (0.229, 0.224, 0.225))\n",
    "])\n",
    "\n",
    "tiny_transform_val = transforms.Compose([\n",
    "    transforms.Resize((image_size, image_size)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.485, 0.456, 0.406),\n",
    "                         (0.229, 0.224, 0.225))\n",
    "])\n",
    "\n",
    "tiny_transform_test = transforms.Compose([\n",
    "    transforms.Resize((image_size, image_size)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.485, 0.456, 0.406),\n",
    "                         (0.229, 0.224, 0.225))\n",
    "])\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "train_loader, val_loader, test_loader= get_tinyimagenet_dataloaders(\n",
    "                                        data_dir = '../datasets',\n",
    "                                        transform_train=tiny_transform_train,\n",
    "                                        transform_val=tiny_transform_val,\n",
    "                                        transform_test=tiny_transform_test,\n",
    "                                        batch_size=batch_size,\n",
    "                                        image_size=image_size)\n",
    "\n",
    "# Define the custom patch embedding class with adjustments\n",
    "class DualPatchEmbedding(nn.Module):\n",
    "    def __init__(self, img_size=224, patch_size1=16, patch_size2=8, embed_dim=768):\n",
    "        super(DualPatchEmbedding, self).__init__()\n",
    "        self.img_size = img_size\n",
    "        self.patch_size1 = patch_size1  # 16\n",
    "        self.patch_size2 = patch_size2  # 8\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "        # Number of patches before rearrangement\n",
    "        self.num_patches1 = (img_size // patch_size1) ** 2  # 14*14=196\n",
    "        self.num_patches2 = (img_size // patch_size2) ** 2  # 28*28=784\n",
    "\n",
    "        # Generate the custom index mapping once\n",
    "        self.idx = self.generate_custom_order()\n",
    "        self.num_patches = len(self.idx)  # Now num_patches = 392 after rearrangement\n",
    "\n",
    "        # Embedding layers for both patch sizes\n",
    "        self.proj1 = nn.Conv2d(3, embed_dim, kernel_size=patch_size1, stride=patch_size1)\n",
    "        self.proj2 = nn.Conv2d(3, embed_dim, kernel_size=patch_size2, stride=patch_size2)\n",
    "\n",
    "        # Positional embeddings\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, self.num_patches + 1, embed_dim))  # +1 for cls token\n",
    "\n",
    "        # Class token\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: [batch_size, 3, img_size, img_size]\n",
    "\n",
    "        # Patch embeddings for 16x16 patches\n",
    "        x1 = self.proj1(x)  # Shape: [batch_size, embed_dim, H1, W1]\n",
    "        x1 = x1.flatten(2).transpose(1, 2)  # Shape: [batch_size, num_patches1, embed_dim]\n",
    "\n",
    "        # Repeat each patch twice to get 392 patches\n",
    "        x1 = x1.repeat_interleave(2, dim=1)  # Now num_patches1 * 2 = 392\n",
    "\n",
    "        # Patch embeddings for 8x8 patches\n",
    "        x2 = self.proj2(x)  # Shape: [batch_size, embed_dim, H2, W2]\n",
    "        x2 = x2.flatten(2).transpose(1, 2)  # Shape: [batch_size, 784, embed_dim]\n",
    "\n",
    "        # Rearrange x2 according to the specified pattern\n",
    "        x2 = self.rearrange_patches(x2)  # Now x2 shape: [batch_size, 392, embed_dim]\n",
    "\n",
    "        # Use the correct positional embeddings\n",
    "        x = x2 + self.pos_embed[:, 1:, :]  # Exclude cls token position\n",
    "        # x shape: [batch_size, 392, embed_dim]\n",
    "\n",
    "        return x, x1, x2\n",
    "\n",
    "    def rearrange_patches(self, x):\n",
    "        x = x[:, self.idx, :]  # Rearranged patches\n",
    "        return x\n",
    "\n",
    "    def generate_custom_order(self):\n",
    "        idx = []\n",
    "        # Adjusted pattern based on your description\n",
    "        for row_block in range(0, 28, 4):\n",
    "            for col_block in range(0, 28, 2):\n",
    "                base = row_block * 28 + col_block\n",
    "                idx.extend([\n",
    "                    base,\n",
    "                    base + 1,\n",
    "                    base + 28,\n",
    "                    base + 29\n",
    "                ])\n",
    "        return idx\n",
    "\n",
    "# Define the modified multi-head attention class\n",
    "class ModifiedMultiheadAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, alpha=0.5, beta=0.5):\n",
    "        super(ModifiedMultiheadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.embed_dim = embed_dim\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        assert (\n",
    "            self.head_dim * num_heads == embed_dim\n",
    "        ), \"embed_dim must be divisible by num_heads\"\n",
    "\n",
    "        # Define linear layers for Q, K, V\n",
    "        self.qkv = nn.Linear(embed_dim, embed_dim * 3, bias=False)\n",
    "\n",
    "        # Output projection\n",
    "        self.out_proj = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "    def forward(self, x, x1, x2):\n",
    "        # x: [batch_size, num_patches+1, embed_dim]\n",
    "        # x1, x2: [batch_size, num_patches+1, embed_dim]\n",
    "\n",
    "        batch_size, num_patches, embed_dim = x.size()\n",
    "\n",
    "        # Compute Q, K, V for both inputs\n",
    "        qkv = self.qkv(x)\n",
    "        qkv1 = self.qkv(x1)\n",
    "        qkv2 = self.qkv(x2)\n",
    "\n",
    "        q, _, _ = qkv.chunk(3, dim=-1)\n",
    "        _, k1, _ = qkv1.chunk(3, dim=-1)\n",
    "        _, k2, v2 = qkv2.chunk(3, dim=-1)  # V comes from x2 (8x8 patches)\n",
    "\n",
    "        # Reshape for multi-head attention\n",
    "        q = q.reshape(batch_size, num_patches, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        k1 = k1.reshape(batch_size, num_patches, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        k2 = k2.reshape(batch_size, num_patches, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        v2 = v2.reshape(batch_size, num_patches, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        # Compute attention scores\n",
    "        attn_scores1 = torch.matmul(q, k1.transpose(-2, -1)) / (self.head_dim ** 0.5)\n",
    "        attn_scores2 = torch.matmul(q, k2.transpose(-2, -1)) / (self.head_dim ** 0.5)\n",
    "\n",
    "        # Combine attention scores\n",
    "        attn_scores = self.alpha * attn_scores1 + self.beta * attn_scores2\n",
    "\n",
    "        # Apply softmax\n",
    "        attn_probs = F.softmax(attn_scores, dim=-1)\n",
    "\n",
    "        # Multiply by V (from x2)\n",
    "        attn_output = torch.matmul(attn_probs, v2)\n",
    "\n",
    "        # Reshape and project output\n",
    "        attn_output = attn_output.transpose(1, 2).reshape(batch_size, num_patches, embed_dim)\n",
    "        attn_output = self.out_proj(attn_output)\n",
    "\n",
    "        return attn_output\n",
    "\n",
    "# Define the modified transformer encoder layer\n",
    "class ModifiedTransformerEncoderLayer(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, mlp_dim, dropout_rate=0.1):\n",
    "        super(ModifiedTransformerEncoderLayer, self).__init__()\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.attn = ModifiedMultiheadAttention(embed_dim, num_heads)\n",
    "        self.dropout1 = nn.Dropout(dropout_rate)\n",
    "\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(embed_dim, mlp_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(mlp_dim, embed_dim),\n",
    "            nn.Dropout(dropout_rate),\n",
    "        )\n",
    "\n",
    "    def forward(self, x, x1, x2):\n",
    "        # x: Input embeddings\n",
    "        # x1, x2: Additional inputs for attention\n",
    "\n",
    "        # Attention block\n",
    "        attn_output = self.attn(self.norm1(x), self.norm1(x1), self.norm1(x2))\n",
    "        x = x + self.dropout1(attn_output)\n",
    "\n",
    "        # MLP block\n",
    "        x = x + self.mlp(self.norm2(x))\n",
    "        return x\n",
    "\n",
    "# Define the modified ViT model with adjustments\n",
    "class ModifiedViT(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        img_size=224,\n",
    "        patch_size1=16,\n",
    "        patch_size2=8,\n",
    "        num_classes=200,\n",
    "        embed_dim=768,\n",
    "        depth=12,\n",
    "        num_heads=12,\n",
    "        mlp_dim=3072,\n",
    "        dropout_rate=0.1,\n",
    "    ):\n",
    "        super(ModifiedViT, self).__init__()\n",
    "\n",
    "        self.patch_embed = DualPatchEmbedding(\n",
    "            img_size=img_size,\n",
    "            patch_size1=patch_size1,\n",
    "            patch_size2=patch_size2,\n",
    "            embed_dim=embed_dim,\n",
    "        )\n",
    "\n",
    "        # The positional embeddings and class token are part of the patch embedding\n",
    "        self.pos_embed = self.patch_embed.pos_embed  # Reuse the positional embeddings\n",
    "        self.cls_token = self.patch_embed.cls_token  # Reuse the class token\n",
    "        self.pos_drop = nn.Dropout(p=dropout_rate)\n",
    "\n",
    "        self.layers = nn.ModuleList(\n",
    "            [\n",
    "                ModifiedTransformerEncoderLayer(\n",
    "                    embed_dim, num_heads, mlp_dim, dropout_rate\n",
    "                )\n",
    "                for _ in range(depth)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "        self.head = nn.Linear(embed_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: [batch_size, 3, img_size, img_size]\n",
    "        x, x1, x2 = self.patch_embed(x)  # Get embeddings and additional inputs\n",
    "\n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        # Concatenate class token\n",
    "        cls_tokens = self.cls_token.expand(batch_size, -1, -1)  # Shape: [batch_size, 1, embed_dim]\n",
    "        x = torch.cat((cls_tokens, x), dim=1)   # x shape: [batch_size, 393, embed_dim]\n",
    "        x1 = torch.cat((cls_tokens, x1), dim=1)  # x1 shape: [batch_size, 393, embed_dim]\n",
    "        x2 = torch.cat((cls_tokens, x2), dim=1)  # x2 shape: [batch_size, 393, embed_dim]\n",
    "\n",
    "        # Add positional embeddings\n",
    "        x = x + self.pos_embed  # x and self.pos_embed should have matching shapes\n",
    "        x = self.pos_drop(x)\n",
    "\n",
    "        # Transformer encoder\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, x1, x2)\n",
    "\n",
    "        x = self.norm(x)\n",
    "\n",
    "        # Classifier head\n",
    "        cls_output = x[:, 0]  # Extract the class token\n",
    "        logits = self.head(cls_output)\n",
    "\n",
    "        return logits\n",
    "\n",
    "# Initialize the modified model\n",
    "model = ModifiedViT(\n",
    "    img_size=224,\n",
    "    patch_size1=16,\n",
    "    patch_size2=8,\n",
    "    num_classes=200,\n",
    "    embed_dim=768,\n",
    "    depth=12,\n",
    "    num_heads=12,\n",
    "    mlp_dim=3072,\n",
    "    dropout_rate=0.1,\n",
    ")\n",
    "\n",
    "# Move the model to the appropriate device\n",
    "model = model.to(device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=0.01)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10)\n",
    "\n",
    "# Define accuracy calculation function\n",
    "def calculate_accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the accuracy over the k top predictions for specified values of k\"\"\"\n",
    "    maxk = max(topk)\n",
    "    batch_size = target.size(0)\n",
    "\n",
    "    # Get the indices of the top k predictions\n",
    "    _, pred = output.topk(maxk, 1, True, True)\n",
    "    pred = pred.t()\n",
    "\n",
    "    # Compare predictions with targets\n",
    "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "    res = []\n",
    "    for k in topk:\n",
    "        # Calculate the number of correct predictions in top k\n",
    "        correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n",
    "        res.append((correct_k / batch_size).item() * 100)\n",
    "    return res  # Returns a list of accuracies\n",
    "\n",
    "# Training and validation loops\n",
    "def train_model(model, criterion, optimizer, scheduler, num_epochs, train_loader, val_loader):\n",
    "    best_acc = 0.0\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'\\nEpoch {epoch+1}/{num_epochs}')\n",
    "        print('-' * 30)\n",
    "\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        top1_acc_train = 0.0\n",
    "        top3_acc_train = 0.0\n",
    "        top5_acc_train = 0.0\n",
    "\n",
    "        for inputs, labels in tqdm(train_loader, desc='Training'):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Statistics\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            acc1, acc3, acc5 = calculate_accuracy(outputs, labels, topk=(1, 3, 5))\n",
    "            top1_acc_train += acc1 * inputs.size(0)\n",
    "            top3_acc_train += acc3 * inputs.size(0)\n",
    "            top5_acc_train += acc5 * inputs.size(0)\n",
    "\n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "        epoch_acc1 = top1_acc_train / len(train_loader.dataset)\n",
    "        epoch_acc3 = top3_acc_train / len(train_loader.dataset)\n",
    "        epoch_acc5 = top5_acc_train / len(train_loader.dataset)\n",
    "\n",
    "        print(f'Train Loss: {epoch_loss:.4f} | Top-1 Acc: {epoch_acc1:.2f}% | Top-3 Acc: {epoch_acc3:.2f}% | Top-5 Acc: {epoch_acc5:.2f}%')\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        running_loss = 0.0\n",
    "        top1_acc_val = 0.0\n",
    "        top3_acc_val = 0.0\n",
    "        top5_acc_val = 0.0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in tqdm(val_loader, desc='Validation'):\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # Forward pass\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                # Statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                acc1, acc3, acc5 = calculate_accuracy(outputs, labels, topk=(1, 3, 5))\n",
    "                top1_acc_val += acc1 * inputs.size(0)\n",
    "                top3_acc_val += acc3 * inputs.size(0)\n",
    "                top5_acc_val += acc5 * inputs.size(0)\n",
    "\n",
    "        epoch_loss = running_loss / len(val_loader.dataset)\n",
    "        epoch_acc1 = top1_acc_val / len(val_loader.dataset)\n",
    "        epoch_acc3 = top3_acc_val / len(val_loader.dataset)\n",
    "        epoch_acc5 = top5_acc_val / len(val_loader.dataset)\n",
    "\n",
    "        print(f'Val Loss: {epoch_loss:.4f} | Top-1 Acc: {epoch_acc1:.2f}% | Top-3 Acc: {epoch_acc3:.2f}% | Top-5 Acc: {epoch_acc5:.2f}%')\n",
    "\n",
    "        # Adjust learning rate\n",
    "        scheduler.step()\n",
    "\n",
    "        # Save the model if it has the best accuracy so far\n",
    "        if epoch_acc1 > best_acc:\n",
    "            best_acc = epoch_acc1\n",
    "            torch.save(model.state_dict(), 'best_modified_vit_model.pth')\n",
    "\n",
    "    print(f'\\nBest Validation Top-1 Accuracy: {best_acc:.2f}%')\n",
    "\n",
    "# Calculate number of parameters\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "# Run the training process\n",
    "num_epochs = 20  # Adjust the number of epochs as needed\n",
    "model_parameters = count_parameters(model)\n",
    "print(f'\\nTotal number of trainable parameters: {model_parameters}')\n",
    "\n",
    "# Test with a dummy input to ensure the model works\n",
    "dummy_input = torch.randn(2, 3, 224, 224).to(device)\n",
    "output = model(dummy_input)\n",
    "print(f'\\nDummy output shape: {output.shape}')\n",
    "\n",
    "# Start training\n",
    "train_model(model, criterion, optimizer, scheduler, num_epochs, train_loader, val_loader)\n",
    "\n",
    "# Evaluate on the test set\n",
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()\n",
    "    top1_acc_test = 0.0\n",
    "    top3_acc_test = 0.0\n",
    "    top5_acc_test = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(test_loader, desc='Testing'):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            acc1, acc3, acc5 = calculate_accuracy(outputs, labels, topk=(1, 3, 5))\n",
    "            top1_acc_test += acc1 * inputs.size(0)\n",
    "            top3_acc_test += acc3 * inputs.size(0)\n",
    "            top5_acc_test += acc5 * inputs.size(0)\n",
    "\n",
    "    epoch_acc1 = top1_acc_test / len(test_loader.dataset)\n",
    "    epoch_acc3 = top3_acc_test / len(test_loader.dataset)\n",
    "    epoch_acc5 = top5_acc_test / len(test_loader.dataset)\n",
    "\n",
    "    print(f'\\nTest Top-1 Accuracy: {epoch_acc1:.2f}%')\n",
    "    print(f'Test Top-3 Accuracy: {epoch_acc3:.2f}%')\n",
    "    print(f'Test Top-5 Accuracy: {epoch_acc5:.2f}%')\n",
    "\n",
    "# Load the best model and evaluate\n",
    "model.load_state_dict(torch.load('best_modified_vit_model.pth'))\n",
    "model = model.to(device)\n",
    "evaluate_model(model, test_loader)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cu12.4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
